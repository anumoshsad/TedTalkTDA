(dp0
S'transcript'
p1
(lp2
(lp3
VI want you to imagine
p4
aVwalking into a room,
p5
aVa control room with a bunch of people,
p6
aVa hundred people, hunched over a desk with little dials,
p7
aVand that that control room
p8
aVwill shape the thoughts and feelings
p9
aVof a billion people.
p10
aVThis might sound like science fiction,
p11
aVbut this actually exists
p12
aVright now, today.
p13
aa(lp14
VI know because I used to be in one of those control rooms.
p15
aVI was a design ethicist at Google,
p16
aVwhere I studied how do you ethically steer people's thoughts?
p17
aVBecause what we don't talk about is how the handful of people
p18
aVworking at a handful of technology companies
p19
aVthrough their choices will steer what a billion people are thinking today.
p20
aVBecause when you pull out your phone
p21
aVand they design how this works or what's on the feed,
p22
aVit's scheduling little blocks of time in our minds.
p23
aVIf you see a notification, it schedules you to have thoughts
p24
aVthat maybe you didn't intend to have.
p25
aVIf you swipe over that notification,
p26
aVit schedules you into spending a little bit of time
p27
aVgetting sucked into something
p28
aVthat maybe you didn't intend to get sucked into.
p29
aVWhen we talk about technology,
p30
aVwe tend to talk about it as this blue sky opportunity.
p31
aVIt could go any direction.
p32
aVAnd I want to get serious for a moment
p33
aVand tell you why it's going in a very specific direction.
p34
aVBecause it's not evolving randomly.
p35
aVThere's a hidden goal driving the direction
p36
aVof all of the technology we make,
p37
aVand that goal is the race for our attention.
p38
aVBecause every new site \u2014
p39
aVTED, elections, politicians,
p40
aVgames, even meditation apps \u2014
p41
aVhave to compete for one thing,
p42
aVwhich is our attention,
p43
aVand there's only so much of it.
p44
aVAnd the best way to get people's attention
p45
aVis to know how someone's mind works.
p46
aVAnd there's a whole bunch of persuasive techniques
p47
aVthat I learned in college at a lab called the Persuasive Technology Lab
p48
aVto get people's attention.
p49
aa(lp50
VA simple example is YouTube.
p51
aVYouTube wants to maximize how much time you spend.
p52
aVAnd so what do they do?
p53
aVThey autoplay the next video.
p54
aVAnd let's say that works really well.
p55
aVThey're getting a little bit more of people's time.
p56
aVWell, if you're Netflix, you look at that and say,
p57
aVwell, that's shrinking my market share,
p58
aVso I'm going to autoplay the next episode.
p59
aVBut then if you're Facebook,
p60
aVyou say, that's shrinking all of my market share,
p61
aVso now I have to autoplay all the videos in the newsfeed
p62
aVbefore waiting for you to click play.
p63
aVSo the internet is not evolving at random.
p64
aVThe reason it feels like it's sucking us in the way it is
p65
aVis because of this race for attention.
p66
aVWe know where this is going.
p67
aVTechnology is not neutral,
p68
aVand it becomes this race to the bottom of the brain stem
p69
aVof who can go lower to get it.
p70
aa(lp71
VLet me give you an example of Snapchat.
p72
aVIf you didn't know, Snapchat is the number one way
p73
aVthat teenagers in the United States communicate.
p74
aVSo if you're like me, and you use text messages to communicate,
p75
aVSnapchat is that for teenagers,
p76
aVand there's, like, a hundred million of them that use it.
p77
aVAnd they invented a feature called Snapstreaks,
p78
aVwhich shows the number of days in a row
p79
aVthat two people have communicated with each other.
p80
aVIn other words, what they just did
p81
aVis they gave two people something they don't want to lose.
p82
aVBecause if you're a teenager, and you have 150 days in a row,
p83
aVyou don't want that to go away.
p84
aVAnd so think of the little blocks of time that that schedules in kids' minds.
p85
aVThis isn't theoretical: when kids go on vacation,
p86
aVit's been shown they give their passwords to up to five other friends
p87
aVto keep their Snapstreaks going,
p88
aVeven when they can't do it.
p89
aVAnd they have, like, 30 of these things,
p90
aVand so they have to get through taking photos of just pictures or walls
p91
aVor ceilings just to get through their day.
p92
aVSo it's not even like they're having real conversations.
p93
aVWe have a temptation to think about this
p94
aVas, oh, they're just using Snapchat
p95
aVthe way we used to gossip on the telephone.
p96
aVIt's probably OK.
p97
aVWell, what this misses is that in the 1970s,
p98
aVwhen you were just gossiping on the telephone,
p99
aVthere wasn't a hundred engineers on the other side of the screen
p100
aVwho knew exactly how your psychology worked
p101
aVand orchestrated you into a double bind with each other.
p102
aa(lp103
VNow, if this is making you feel a little bit of outrage,
p104
aVnotice that that thought just comes over you.
p105
aVOutrage is a really good way also of getting your attention,
p106
aVbecause we don't choose outrage.
p107
aVIt happens to us.
p108
aVAnd if you're the Facebook newsfeed,
p109
aVwhether you'd want to or not,
p110
aVyou actually benefit when there's outrage.
p111
aVBecause outrage doesn't just schedule a reaction
p112
aVin emotional time, space, for you.
p113
aVWe want to share that outrage with other people.
p114
aVSo we want to hit share and say,
p115
aV"Can you believe the thing that they said?"
p116
aVAnd so outrage works really well at getting attention,
p117
aVsuch that if Facebook had a choice between showing you the outrage feed
p118
aVand a calm newsfeed,
p119
aVthey would want to show you the outrage feed,
p120
aVnot because someone consciously chose that,
p121
aVbut because that worked better at getting your attention.
p122
aVAnd the newsfeed control room is not accountable to us.
p123
aVIt's only accountable to maximizing attention.
p124
aVIt's also accountable,
p125
aVbecause of the business model of advertising,
p126
aVfor anybody who can pay the most to actually walk into the control room
p127
aVand say, "That group over there,
p128
aVI want to schedule these thoughts into their minds."
p129
aVSo you can target,
p130
aVyou can precisely target a lie
p131
aVdirectly to the people who are most susceptible.
p132
aVAnd because this is profitable, it's only going to get worse.
p133
aa(lp134
VSo I'm here today
p135
aVbecause the costs are so obvious.
p136
aVI don't know a more urgent problem than this,
p137
aVbecause this problem is underneath all other problems.
p138
aVIt's not just taking away our agency
p139
aVto spend our attention and live the lives that we want,
p140
aVit's changing the way that we have our conversations,
p141
aVit's changing our democracy,
p142
aVand it's changing our ability to have the conversations
p143
aVand relationships we want with each other.
p144
aVAnd it affects everyone,
p145
aVbecause a billion people have one of these in their pocket.
p146
aa(lp147
VSo how do we fix this?
p148
aVWe need to make three radical changes
p149
aVto technology and to our society.
p150
aVThe first is we need to acknowledge that we are persuadable.
p151
aVOnce you start understanding
p152
aVthat your mind can be scheduled into having little thoughts
p153
aVor little blocks of time that you didn't choose,
p154
aVwouldn't we want to use that understanding
p155
aVand protect against the way that that happens?
p156
aVI think we need to see ourselves fundamentally in a new way.
p157
aVIt's almost like a new period of human history,
p158
aVlike the Enlightenment,
p159
aVbut almost a kind of self-aware Enlightenment,
p160
aVthat we can be persuaded,
p161
aVand there might be something we want to protect.
p162
aVThe second is we need new models and accountability systems
p163
aVso that as the world gets better and more and more persuasive over time \u2014
p164
aVbecause it's only going to get more persuasive \u2014
p165
aVthat the people in those control rooms
p166
aVare accountable and transparent to what we want.
p167
aVThe only form of ethical persuasion that exists
p168
aVis when the goals of the persuader
p169
aVare aligned with the goals of the persuadee.
p170
aVAnd that involves questioning big things, like the business model of advertising.
p171
aVLastly,
p172
aVwe need a design renaissance,
p173
aVbecause once you have this view of human nature,
p174
aVthat you can steer the timelines of a billion people \u2014
p175
aVjust imagine, there's people who have some desire
p176
aVabout what they want to do and what they want to be thinking
p177
aVand what they want to be feeling and how they want to be informed,
p178
aVand we're all just tugged into these other directions.
p179
aVAnd you have a billion people just tugged into all these different directions.
p180
aVWell, imagine an entire design renaissance
p181
aVthat tried to orchestrate the exact and most empowering
p182
aVtime-well-spent way for those timelines to happen.
p183
aVAnd that would involve two things:
p184
aVone would be protecting against the timelines
p185
aVthat we don't want to be experiencing,
p186
aVthe thoughts that we wouldn't want to be happening,
p187
aVso that when that ding happens, not having the ding that sends us away;
p188
aVand the second would be empowering us to live out the timeline that we want.
p189
aa(lp190
VSo let me give you a concrete example.
p191
aVToday, let's say your friend cancels dinner on you,
p192
aVand you are feeling a little bit lonely.
p193
aVAnd so what do you do in that moment?
p194
aVYou open up Facebook.
p195
aVAnd in that moment,
p196
aVthe designers in the control room want to schedule exactly one thing,
p197
aVwhich is to maximize how much time you spend on the screen.
p198
aVNow, instead, imagine if those designers created a different timeline
p199
aVthat was the easiest way, using all of their data,
p200
aVto actually help you get out with the people that you care about?
p201
aVJust think, alleviating all loneliness in society,
p202
aVif that was the timeline that Facebook wanted to make possible for people.
p203
aVOr imagine a different conversation.
p204
aVLet's say you wanted to post something supercontroversial on Facebook,
p205
aVwhich is a really important thing to be able to do,
p206
aVto talk about controversial topics.
p207
aVAnd right now, when there's that big comment box,
p208
aVit's almost asking you, what key do you want to type?
p209
aVIn other words, it's scheduling a little timeline of things
p210
aVyou're going to continue to do on the screen.
p211
aVAnd imagine instead that there was another button there saying,
p212
aVwhat would be most time well spent for you?
p213
aVAnd you click "host a dinner."
p214
aVAnd right there underneath the item it said,
p215
aV"Who wants to RSVP for the dinner?"
p216
aVAnd so you'd still have a conversation about something controversial,
p217
aVbut you'd be having it in the most empowering place on your timeline,
p218
aVwhich would be at home that night with a bunch of a friends over
p219
aVto talk about it.
p220
aVSo imagine we're running, like, a find and replace
p221
aVon all of the timelines that are currently steering us
p222
aVtowards more and more screen time persuasively
p223
aVand replacing all of those timelines
p224
aVwith what do we want in our lives.
p225
aa(lp226
VIt doesn't have to be this way.
p227
aVInstead of handicapping our attention,
p228
aVimagine if we used all of this data and all of this power
p229
aVand this new view of human nature
p230
aVto give us a superhuman ability to focus
p231
aVand a superhuman ability to put our attention to what we cared about
p232
aVand a superhuman ability to have the conversations
p233
aVthat we need to have for democracy.
p234
aVThe most complex challenges in the world
p235
aVrequire not just us to use our attention individually.
p236
aVThey require us to use our attention and coordinate it together.
p237
aVClimate change is going to require that a lot of people
p238
aVare being able to coordinate their attention
p239
aVin the most empowering way together.
p240
aVAnd imagine creating a superhuman ability to do that.
p241
aa(lp242
VSometimes the world's most pressing and important problems
p243
aVare not these hypothetical future things that we could create in the future.
p244
aVSometimes the most pressing problems
p245
aVare the ones that are right underneath our noses,
p246
aVthe things that are already directing a billion people's thoughts.
p247
aVAnd maybe instead of getting excited about the new augmented reality
p248
aVand virtual reality and these cool things that could happen,
p249
aVwhich are going to be susceptible to the same race for attention,
p250
aVif we could fix the race for attention
p251
aVon the thing that's already in a billion people's pockets.
p252
aVMaybe instead of getting excited
p253
aVabout the most exciting new cool fancy education apps,
p254
aVwe could fix the way kids' minds are getting manipulated
p255
aVinto sending empty messages back and forth.
p256
aa(lp257
V(Applause)
p258
aa(lp259
VMaybe instead of worrying
p260
aVabout hypothetical future runaway artificial intelligences
p261
aVthat are maximizing for one goal,
p262
aVwe could solve the runaway artificial intelligence
p263
aVthat already exists right now,
p264
aVwhich are these newsfeeds maximizing for one thing.
p265
aVIt's almost like instead of running away to colonize new planets,
p266
aVwe could fix the one that we're already on.
p267
aa(lp268
V(Applause)
p269
aa(lp270
VSolving this problem
p271
aVis critical infrastructure for solving every other problem.
p272
aVThere's nothing in your life or in our collective problems
p273
aVthat does not require our ability to put our attention where we care about.
p274
aVAt the end of our lives,
p275
aVall we have is our attention and our time.
p276
aVWhat will be time well spent for ours?
p277
aa(lp278
VThank you.
p279
aa(lp280
V(Applause)
p281
aa(lp282
VChris Anderson: Tristan, thank you. Hey, stay up here a sec.
p283
aVFirst of all, thank you.
p284
aVI know we asked you to do this talk on pretty short notice,
p285
aVand you've had quite a stressful week
p286
aVgetting this thing together, so thank you.
p287
aVSome people listening might say, what you complain about is addiction,
p288
aVand all these people doing this stuff, for them it's actually interesting.
p289
aVAll these design decisions
p290
aVhave built user content that is fantastically interesting.
p291
aVThe world's more interesting than it ever has been.
p292
aVWhat's wrong with that?
p293
aa(lp294
VTristan Harris: I think it's really interesting.
p295
aVOne way to see this is if you're just YouTube, for example,
p296
aVyou want to always show the more interesting next video.
p297
aVYou want to get better and better at suggesting that next video,
p298
aVbut even if you could propose the perfect next video
p299
aVthat everyone would want to watch,
p300
aVit would just be better and better at keeping you hooked on the screen.
p301
aVSo what's missing in that equation
p302
aVis figuring out what our boundaries would be.
p303
aVYou would want YouTube to know something about, say, falling asleep.
p304
aVThe CEO of Netflix recently said,
p305
aV"our biggest competitors are Facebook, YouTube and sleep."
p306
aVAnd so what we need to recognize is that the human architecture is limited
p307
aVand that we have certain boundaries or dimensions of our lives
p308
aVthat we want to be honored and respected,
p309
aVand technology could help do that.
p310
aa(lp311
V(Applause)
p312
aa(lp313
VCA: I mean, could you make the case
p314
aVthat part of the problem here is that we've got a naïve model of human nature?
p315
aVSo much of this is justified in terms of human preference,
p316
aVwhere we've got these algorithms that do an amazing job
p317
aVof optimizing for human preference,
p318
aVbut which preference?
p319
aVThere's the preferences of things that we really care about
p320
aVwhen we think about them
p321
aVversus the preferences of what we just instinctively click on.
p322
aVIf we could implant that more nuanced view of human nature in every design,
p323
aVwould that be a step forward?
p324
aa(lp325
VTH: Absolutely. I mean, I think right now
p326
aVit's as if all of our technology is basically only asking our lizard brain
p327
aVwhat's the best way to just impulsively get you to do
p328
aVthe next tiniest thing with your time,
p329
aVinstead of asking you in your life
p330
aVwhat we would be most time well spent for you?
p331
aVWhat would be the perfect timeline that might include something later,
p332
aVwould be time well spent for you here at TED in your last day here?
p333
aa(lp334
VCA: So if Facebook and Google and everyone said to us first up,
p335
aV"Hey, would you like us to optimize for your reflective brain
p336
aVor your lizard brain? You choose."
p337
aa(lp338
VTH: Right. That would be one way. Yes.
p339
aa(lp340
VCA: You said persuadability, that's an interesting word to me
p341
aVbecause to me there's two different types of persuadability.
p342
aVThere's the persuadability that we're trying right now
p343
aVof reason and thinking and making an argument,
p344
aVbut I think you're almost talking about a different kind,
p345
aVa more visceral type of persuadability,
p346
aVof being persuaded without even knowing that you're thinking.
p347
aa(lp348
VTH: Exactly. The reason I care about this problem so much is
p349
aVI studied at a lab called the Persuasive Technology Lab at Stanford
p350
aVthat taught [students how to recognize] exactly these techniques.
p351
aVThere's conferences and workshops that teach people all these covert ways
p352
aVof getting people's attention and orchestrating people's lives.
p353
aVAnd it's because most people don't know that that exists
p354
aVthat this conversation is so important.
p355
aa(lp356
VCA: Tristan, you and I, we both know so many people from all these companies.
p357
aVThere are actually many here in the room,
p358
aVand I don't know about you, but my experience of them
p359
aVis that there is no shortage of good intent.
p360
aVPeople want a better world.
p361
aVThey are actually \u2014 they really want it.
p362
aVAnd I don't think anything you're saying is that these are evil people.
p363
aVIt's a system where there's these unintended consequences
p364
aVthat have really got out of control \u2014
p365
aa(lp366
VTH: Of this race for attention.
p367
aVIt's the classic race to the bottom when you have to get attention,
p368
aVand it's so tense.
p369
aVThe only way to get more is to go lower on the brain stem,
p370
aVto go lower into outrage, to go lower into emotion,
p371
aVto go lower into the lizard brain.
p372
aa(lp373
VCA: Well, thank you so much for helping us all get a little bit wiser about this.
p374
aa(lp375
VTristan Harris, thank you. TH: Thank you very much.
p376
aa(lp377
V(Applause)
p378
aasS'id'
p379
I2802
sS'title'
p380
VHow a handful of tech companies control billions of minds every day
p381
s.