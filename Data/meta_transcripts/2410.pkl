(dp0
S'transcript'
p1
(lp2
(lp3
VWe've evolved with tools, and tools have evolved with us.
p4
aVOur ancestors created these hand axes 1.5 million years ago,
p5
aVshaping them to not only fit the task at hand
p6
aVbut also their hand.
p7
aa(lp8
VHowever, over the years,
p9
aVtools have become more and more specialized.
p10
aVThese sculpting tools have evolved through their use,
p11
aVand each one has a different form which matches its function.
p12
aVAnd they leverage the dexterity of our hands
p13
aVin order to manipulate things with much more precision.
p14
aVBut as tools have become more and more complex,
p15
aVwe need more complex controls to control them.
p16
aVAnd so designers have become very adept at creating interfaces
p17
aVthat allow you to manipulate parameters while you're attending to other things,
p18
aVsuch as taking a photograph and changing the focus
p19
aVor the aperture.
p20
aa(lp21
VBut the computer has fundamentally changed the way we think about tools
p22
aVbecause computation is dynamic.
p23
aVSo it can do a million different things
p24
aVand run a million different applications.
p25
aVHowever, computers have the same static physical form
p26
aVfor all of these different applications
p27
aVand the same static interface elements as well.
p28
aVAnd I believe that this is fundamentally a problem,
p29
aVbecause it doesn't really allow us to interact with our hands
p30
aVand capture the rich dexterity that we have in our bodies.
p31
aVAnd my belief is that, then, we must need new types of interfaces
p32
aVthat can capture these rich abilities that we have
p33
aVand that can physically adapt to us
p34
aVand allow us to interact in new ways.
p35
aVAnd so that's what I've been doing at the MIT Media Lab
p36
aVand now at Stanford.
p37
aa(lp38
VSo with my colleagues, Daniel Leithinger and Hiroshi Ishii,
p39
aVwe created inFORM,
p40
aVwhere the interface can actually come off the screen
p41
aVand you can physically manipulate it.
p42
aVOr you can visualize 3D information physically
p43
aVand touch it and feel it to understand it in new ways.
p44
aVOr you can interact through gestures and direct deformations
p45
aVto sculpt digital clay.
p46
aVOr interface elements can arise out of the surface
p47
aVand change on demand.
p48
aVAnd the idea is that for each individual application,
p49
aVthe physical form can be matched to the application.
p50
aVAnd I believe this represents a new way
p51
aVthat we can interact with information,
p52
aVby making it physical.
p53
aa(lp54
VSo the question is, how can we use this?
p55
aVTraditionally, urban planners and architects build physical models
p56
aVof cities and buildings to better understand them.
p57
aVSo with Tony Tang at the Media Lab, we created an interface built on inFORM
p58
aVto allow urban planners to design and view entire cities.
p59
aVAnd now you can walk around it, but it's dynamic, it's physical,
p60
aVand you can also interact directly.
p61
aVOr you can look at different views,
p62
aVsuch as population or traffic information,
p63
aVbut it's made physical.
p64
aa(lp65
VWe also believe that these dynamic shape displays can really change
p66
aVthe ways that we remotely collaborate with people.
p67
aVSo when we're working together in person,
p68
aVI'm not only looking at your face
p69
aVbut I'm also gesturing and manipulating objects,
p70
aVand that's really hard to do when you're using tools like Skype.
p71
aVAnd so using inFORM, you can reach out from the screen
p72
aVand manipulate things at a distance.
p73
aVSo we used the pins of the display to represent people's hands,
p74
aVallowing them to actually touch and manipulate objects at a distance.
p75
aVAnd you can also manipulate and collaborate on 3D data sets as well,
p76
aVso you can gesture around them as well as manipulate them.
p77
aVAnd that allows people to collaborate on these new types of 3D information
p78
aVin a richer way than might be possible with traditional tools.
p79
aVAnd so you can also bring in existing objects,
p80
aVand those will be captured on one side and transmitted to the other.
p81
aVOr you can have an object that's linked between two places,
p82
aVso as I move a ball on one side,
p83
aVthe ball moves on the other as well.
p84
aVAnd so we do this by capturing the remote user
p85
aVusing a depth-sensing camera like a Microsoft Kinect.
p86
aa(lp87
VNow, you might be wondering how does this all work,
p88
aVand essentially, what it is, is 900 linear actuators
p89
aVthat are connected to these mechanical linkages
p90
aVthat allow motion down here to be propagated in these pins above.
p91
aVSo it's not that complex compared to what's going on at CERN,
p92
aVbut it did take a long time for us to build it.
p93
aVAnd so we started with a single motor,
p94
aVa single linear actuator,
p95
aVand then we had to design a custom circuit board to control them.
p96
aVAnd then we had to make a lot of them.
p97
aVAnd so the problem with having 900 of something
p98
aVis that you have to do every step 900 times.
p99
aVAnd so that meant that we had a lot of work to do.
p100
aVSo we sort of set up a mini-sweatshop in the Media Lab
p101
aVand brought undergrads in and convinced them to do "research" \u2014
p102
aa(lp103
V(Laughter)
p104
aa(lp105
Vand had late nights watching movies, eating pizza
p106
aVand screwing in thousands of screws.
p107
aVYou know \u2014 research.
p108
aa(lp109
V(Laughter)
p110
aa(lp111
VBut anyway, I think that we were really excited by the things
p112
aVthat inFORM allowed us to do.
p113
aVIncreasingly, we're using mobile devices, and we interact on the go.
p114
aVBut mobile devices, just like computers,
p115
aVare used for so many different applications.
p116
aVSo you use them to talk on the phone,
p117
aVto surf the web, to play games, to take pictures
p118
aVor even a million different things.
p119
aVBut again, they have the same static physical form
p120
aVfor each of these applications.
p121
aVAnd so we wanted to know how can we take some of the same interactions
p122
aVthat we developed for inFORM
p123
aVand bring them to mobile devices.
p124
aa(lp125
VSo at Stanford, we created this haptic edge display,
p126
aVwhich is a mobile device with an array of linear actuators
p127
aVthat can change shape,
p128
aVso you can feel in your hand where you are as you're reading a book.
p129
aVOr you can feel in your pocket new types of tactile sensations
p130
aVthat are richer than the vibration.
p131
aVOr buttons can emerge from the side that allow you to interact
p132
aVwhere you want them to be.
p133
aVOr you can play games and have actual buttons.
p134
aVAnd so we were able to do this
p135
aVby embedding 40 small, tiny linear actuators inside the device,
p136
aVand that allow you not only to touch them
p137
aVbut also back-drive them as well.
p138
aa(lp139
VBut we've also looked at other ways to create more complex shape change.
p140
aVSo we've used pneumatic actuation to create a morphing device
p141
aVwhere you can go from something that looks a lot like a phone ...
p142
aVto a wristband on the go.
p143
aVAnd so together with Ken Nakagaki at the Media Lab,
p144
aVwe created this new high-resolution version
p145
aVthat uses an array of servomotors to change from interactive wristband
p146
aVto a touch-input device
p147
aVto a phone.
p148
aa(lp149
V(Laughter)
p150
aa(lp151
VAnd we're also interested in looking at ways
p152
aVthat users can actually deform the interfaces
p153
aVto shape them into the devices that they want to use.
p154
aVSo you can make something like a game controller,
p155
aVand then the system will understand what shape it's in
p156
aVand change to that mode.
p157
aa(lp158
VSo, where does this point?
p159
aVHow do we move forward from here?
p160
aVI think, really, where we are today
p161
aVis in this new age of the Internet of Things,
p162
aVwhere we have computers everywhere \u2014
p163
aVthey're in our pockets, they're in our walls,
p164
aVthey're in almost every device that you'll buy in the next five years.
p165
aVBut what if we stopped thinking about devices
p166
aVand think instead about environments?
p167
aVAnd so how can we have smart furniture
p168
aVor smart rooms or smart environments
p169
aVor cities that can adapt to us physically,
p170
aVand allow us to do new ways of collaborating with people
p171
aVand doing new types of tasks?
p172
aa(lp173
VSo for the Milan Design Week, we created TRANSFORM,
p174
aVwhich is an interactive table-scale version of these shape displays,
p175
aVwhich can move physical objects on the surface; for example,
p176
aVreminding you to take your keys.
p177
aVBut it can also transform to fit different ways of interacting.
p178
aVSo if you want to work,
p179
aVthen it can change to sort of set up your work system.
p180
aVAnd so as you bring a device over,
p181
aVit creates all the affordances you need
p182
aVand brings other objects to help you accomplish those goals.
p183
aa(lp184
VSo, in conclusion,
p185
aVI really think that we need to think about a new, fundamentally different way
p186
aVof interacting with computers.
p187
aVWe need computers that can physically adapt to us
p188
aVand adapt to the ways that we want to use them
p189
aVand really harness the rich dexterity that we have of our hands,
p190
aVand our ability to think spatially about information by making it physical.
p191
aVBut looking forward, I think we need to go beyond this, beyond devices,
p192
aVto really think about new ways that we can bring people together,
p193
aVand bring our information into the world,
p194
aVand think about smart environments that can adapt to us physically.
p195
aVSo with that, I will leave you.
p196
aa(lp197
VThank you very much.
p198
aa(lp199
V(Applause)
p200
aasS'id'
p201
I2410
sS'title'
p202
VShape-shifting tech will change work as we know it
p203
s.