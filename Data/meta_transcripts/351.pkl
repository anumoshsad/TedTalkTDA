(dp0
S'transcript'
p1
(lp2
(lp3
VIf you ask people about what part of psychology do they think is hard,
p4
aVand you say, "Well, what about thinking and emotions?"
p5
aVMost people will say, "Emotions are terribly hard.
p6
aVThey're incredibly complex. They can't \u2014 I have no idea of how they work.
p7
aVBut thinking is really very straightforward:
p8
aVit's just sort of some kind of logical reasoning, or something.
p9
aVBut that's not the hard part."
p10
aa(lp11
VSo here's a list of problems that come up.
p12
aVOne nice problem is, what do we do about health?
p13
aVThe other day, I was reading something, and the person said
p14
aVprobably the largest single cause of disease is handshaking in the West.
p15
aVAnd there was a little study about people who don't handshake,
p16
aVand comparing them with ones who do handshake.
p17
aVAnd I haven't the foggiest idea of where you find the ones that don't handshake,
p18
aVbecause they must be hiding.
p19
aVAnd the people who avoid that
p20
aVhave 30 percent less infectious disease or something.
p21
aVOr maybe it was 31 and a quarter percent.
p22
aVSo if you really want to solve the problem of epidemics and so forth,
p23
aVlet's start with that. And since I got that idea,
p24
aVI've had to shake hundreds of hands.
p25
aVAnd I think the only way to avoid it
p26
aVis to have some horrible visible disease,
p27
aVand then you don't have to explain.
p28
aa(lp29
VEducation: how do we improve education?
p30
aVWell, the single best way is to get them to understand
p31
aVthat what they're being told is a whole lot of nonsense.
p32
aVAnd then, of course, you have to do something
p33
aVabout how to moderate that, so that anybody can \u2014 so they'll listen to you.
p34
aVPollution, energy shortage, environmental diversity, poverty.
p35
aVHow do we make stable societies? Longevity.
p36
aVOkay, there're lots of problems to worry about.
p37
aa(lp38
VAnyway, the question I think people should talk about \u2014
p39
aVand it's absolutely taboo \u2014 is, how many people should there be?
p40
aVAnd I think it should be about 100 million or maybe 500 million.
p41
aVAnd then notice that a great many of these problems disappear.
p42
aVIf you had 100 million people
p43
aVproperly spread out, then if there's some garbage,
p44
aVyou throw it away, preferably where you can't see it, and it will rot.
p45
aVOr you throw it into the ocean and some fish will benefit from it.
p46
aVThe problem is, how many people should there be?
p47
aVAnd it's a sort of choice we have to make.
p48
aa(lp49
VMost people are about 60 inches high or more,
p50
aVand there's these cube laws. So if you make them this big,
p51
aVby using nanotechnology, I suppose \u2014
p52
aV(Laughter)
p53
aV\u2014 then you could have a thousand times as many.
p54
aVThat would solve the problem, but I don't see anybody
p55
aVdoing any research on making people smaller.
p56
aVNow, it's nice to reduce the population, but a lot of people want to have children.
p57
aVAnd there's one solution that's probably only a few years off.
p58
aVYou know you have 46 chromosomes. If you're lucky, you've got 23
p59
aVfrom each parent. Sometimes you get an extra one or drop one out,
p60
aVbut \u2014 so you can skip the grandparent and great-grandparent stage
p61
aVand go right to the great-great-grandparent. And you have 46 people
p62
aVand you give them a scanner, or whatever you need,
p63
aVand they look at their chromosomes and each of them says
p64
aVwhich one he likes best, or she \u2014 no reason to have just two sexes
p65
aVany more, even. So each child has 46 parents,
p66
aVand I suppose you could let each group of 46 parents have 15 children.
p67
aVWouldn't that be enough? And then the children
p68
aVwould get plenty of support, and nurturing, and mentoring,
p69
aVand the world population would decline very rapidly
p70
aVand everybody would be totally happy.
p71
aa(lp72
VTimesharing is a little further off in the future.
p73
aVAnd there's this great novel that Arthur Clarke wrote twice,
p74
aVcalled "Against the Fall of Night" and "The City and the Stars."
p75
aVThey're both wonderful and largely the same,
p76
aVexcept that computers happened in between.
p77
aVAnd Arthur was looking at this old book, and he said, "Well, that was wrong.
p78
aVThe future must have some computers."
p79
aVSo in the second version of it, there are 100 billion
p80
aVor 1,000 billion people on Earth, but they're all stored on hard disks or floppies,
p81
aVor whatever they have in the future.
p82
aVAnd you let a few million of them out at a time.
p83
aVA person comes out, they live for a thousand years
p84
aVdoing whatever they do, and then, when it's time to go back
p85
aVfor a billion years \u2014 or a million, I forget, the numbers don't matter \u2014
p86
aVbut there really aren't very many people on Earth at a time.
p87
aVAnd you get to think about yourself and your memories,
p88
aVand before you go back into suspension, you edit your memories
p89
aVand you change your personality and so forth.
p90
aVThe plot of the book is that there's not enough diversity,
p91
aVso that the people who designed the city
p92
aVmake sure that every now and then an entirely new person is created.
p93
aVAnd in the novel, a particular one named Alvin is created. And he says,
p94
aVmaybe this isn't the best way, and wrecks the whole system.
p95
aa(lp96
VI don't think the solutions that I proposed
p97
aVare good enough or smart enough.
p98
aVI think the big problem is that we're not smart enough
p99
aVto understand which of the problems we're facing are good enough.
p100
aVTherefore, we have to build super intelligent machines like HAL.
p101
aVAs you remember, at some point in the book for "2001,"
p102
aVHAL realizes that the universe is too big, and grand, and profound
p103
aVfor those really stupid astronauts. If you contrast HAL's behavior
p104
aVwith the triviality of the people on the spaceship,
p105
aVyou can see what's written between the lines.
p106
aVWell, what are we going to do about that? We could get smarter.
p107
aVI think that we're pretty smart, as compared to chimpanzees,
p108
aVbut we're not smart enough to deal with the colossal problems that we face,
p109
aVeither in abstract mathematics
p110
aVor in figuring out economies, or balancing the world around.
p111
aVSo one thing we can do is live longer.
p112
aVAnd nobody knows how hard that is,
p113
aVbut we'll probably find out in a few years.
p114
aVYou see, there's two forks in the road. We know that people live
p115
aVtwice as long as chimpanzees almost,
p116
aVand nobody lives more than 120 years,
p117
aVfor reasons that aren't very well understood.
p118
aVBut lots of people now live to 90 or 100,
p119
aVunless they shake hands too much or something like that.
p120
aVAnd so maybe if we lived 200 years, we could accumulate enough skills
p121
aVand knowledge to solve some problems.
p122
aVSo that's one way of going about it.
p123
aVAnd as I said, we don't know how hard that is. It might be \u2014
p124
aVafter all, most other mammals live half as long as the chimpanzee,
p125
aVso we're sort of three and a half or four times, have four times
p126
aVthe longevity of most mammals. And in the case of the primates,
p127
aVwe have almost the same genes. We only differ from chimpanzees,
p128
aVin the present state of knowledge, which is absolute hogwash,
p129
aVmaybe by just a few hundred genes.
p130
aa(lp131
VWhat I think is that the gene counters don't know what they're doing yet.
p132
aVAnd whatever you do, don't read anything about genetics
p133
aVthat's published within your lifetime, or something.
p134
aV(Laughter)
p135
aVThe stuff has a very short half-life, same with brain science.
p136
aVAnd so it might be that if we just fix four or five genes,
p137
aVwe can live 200 years.
p138
aVOr it might be that it's just 30 or 40,
p139
aVand I doubt that it's several hundred.
p140
aVSo this is something that people will be discussing
p141
aVand lots of ethicists \u2014 you know, an ethicist is somebody
p142
aVwho sees something wrong with whatever you have in mind.
p143
aV(Laughter)
p144
aVAnd it's very hard to find an ethicist who considers any change
p145
aVworth making, because he says, what about the consequences?
p146
aVAnd, of course, we're not responsible for the consequences
p147
aVof what we're doing now, are we? Like all this complaint about clones.
p148
aVAnd yet two random people will mate and have this child,
p149
aVand both of them have some pretty rotten genes,
p150
aVand the child is likely to come out to be average.
p151
aVWhich, by chimpanzee standards, is very good indeed.
p152
aa(lp153
VIf we do have longevity, then we'll have to face the population growth
p154
aVproblem anyway. Because if people live 200 or 1,000 years,
p155
aVthen we can't let them have a child more than about once every 200 or 1,000 years.
p156
aVAnd so there won't be any workforce.
p157
aVAnd one of the things Laurie Garrett pointed out, and others have,
p158
aVis that a society that doesn't have people
p159
aVof working age is in real trouble. And things are going to get worse,
p160
aVbecause there's nobody to educate the children or to feed the old.
p161
aVAnd when I'm talking about a long lifetime, of course,
p162
aVI don't want somebody who's 200 years old to be like our image
p163
aVof what a 200-year-old is \u2014 which is dead, actually.
p164
aa(lp165
VYou know, there's about 400 different parts of the brain
p166
aVwhich seem to have different functions.
p167
aVNobody knows how most of them work in detail,
p168
aVbut we do know that there're lots of different things in there.
p169
aVAnd they don't always work together. I like Freud's theory
p170
aVthat most of them are cancelling each other out.
p171
aVAnd so if you think of yourself as a sort of city
p172
aVwith a hundred resources, then, when you're afraid, for example,
p173
aVyou may discard your long-range goals, but you may think deeply
p174
aVand focus on exactly how to achieve that particular goal.
p175
aVYou throw everything else away. You become a monomaniac \u2014
p176
aVall you care about is not stepping out on that platform.
p177
aVAnd when you're hungry, food becomes more attractive, and so forth.
p178
aVSo I see emotions as highly evolved subsets of your capability.
p179
aVEmotion is not something added to thought. An emotional state
p180
aVis what you get when you remove 100 or 200
p181
aVof your normally available resources.
p182
aa(lp183
VSo thinking of emotions as the opposite of \u2014 as something
p184
aVless than thinking is immensely productive. And I hope,
p185
aVin the next few years, to show that this will lead to smart machines.
p186
aVAnd I guess I better skip all the rest of this, which are some details
p187
aVon how we might make those smart machines and \u2014
p188
aV(Laughter)
p189
aV\u2014 and the main idea is in fact that the core of a really smart machine
p190
aVis one that recognizes that a certain kind of problem is facing you.
p191
aVThis is a problem of such and such a type,
p192
aVand therefore there's a certain way or ways of thinking
p193
aVthat are good for that problem.
p194
aVSo I think the future, main problem of psychology is to classify
p195
aVtypes of predicaments, types of situations, types of obstacles
p196
aVand also to classify available and possible ways to think and pair them up.
p197
aVSo you see, it's almost like a Pavlovian \u2014
p198
aVwe lost the first hundred years of psychology
p199
aVby really trivial theories, where you say,
p200
aVhow do people learn how to react to a situation? What I'm saying is,
p201
aVafter we go through a lot of levels, including designing
p202
aVa huge, messy system with thousands of ports,
p203
aVwe'll end up again with the central problem of psychology.
p204
aVSaying, not what are the situations,
p205
aVbut what are the kinds of problems
p206
aVand what are the kinds of strategies, how do you learn them,
p207
aVhow do you connect them up, how does a really creative person
p208
aVinvent a new way of thinking out of the available resources and so forth.
p209
aa(lp210
VSo, I think in the next 20 years,
p211
aVif we can get rid of all of the traditional approaches to artificial intelligence,
p212
aVlike neural nets and genetic algorithms
p213
aVand rule-based systems, and just turn our sights a little bit higher to say,
p214
aVcan we make a system that can use all those things
p215
aVfor the right kind of problem? Some problems are good for neural nets;
p216
aVwe know that others, neural nets are hopeless on them.
p217
aVGenetic algorithms are great for certain things;
p218
aVI suspect I know what they're bad at, and I won't tell you.
p219
aV(Laughter)
p220
aa(lp221
VThank you.
p222
aV(Applause)
p223
aasS'id'
p224
I351
sS'title'
p225
VHealth and the human mind
p226
s.