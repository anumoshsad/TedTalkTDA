(dp0
S'transcript'
p1
(lp2
(lp3
VI would like to tell you a story
p4
aVconnecting the notorious privacy incident
p5
aVinvolving Adam and Eve,
p6
aVand the remarkable shift in the boundaries
p7
aVbetween public and private which has occurred
p8
aVin the past 10 years.
p9
aa(lp10
VYou know the incident.
p11
aVAdam and Eve one day in the Garden of Eden
p12
aVrealize they are naked.
p13
aVThey freak out.
p14
aVAnd the rest is history.
p15
aa(lp16
VNowadays, Adam and Eve
p17
aVwould probably act differently.
p18
aa(lp19
V[@Adam Last nite was a blast! loved dat apple LOL]
p20
aa(lp21
V[@Eve yep.. babe, know what happened to my pants tho?]
p22
aa(lp23
VWe do reveal so much more information
p24
aVabout ourselves online than ever before,
p25
aVand so much information about us
p26
aVis being collected by organizations.
p27
aVNow there is much to gain and benefit
p28
aVfrom this massive analysis of personal information,
p29
aVor big data,
p30
aVbut there are also complex tradeoffs that come
p31
aVfrom giving away our privacy.
p32
aVAnd my story is about these tradeoffs.
p33
aa(lp34
VWe start with an observation which, in my mind,
p35
aVhas become clearer and clearer in the past few years,
p36
aVthat any personal information
p37
aVcan become sensitive information.
p38
aVBack in the year 2000, about 100 billion photos
p39
aVwere shot worldwide,
p40
aVbut only a minuscule proportion of them
p41
aVwere actually uploaded online.
p42
aVIn 2010, only on Facebook, in a single month,
p43
aV2.5 billion photos were uploaded,
p44
aVmost of them identified.
p45
aVIn the same span of time,
p46
aVcomputers' ability to recognize people in photos
p47
aVimproved by three orders of magnitude.
p48
aVWhat happens when you combine
p49
aVthese technologies together:
p50
aVincreasing availability of facial data;
p51
aVimproving facial recognizing ability by computers;
p52
aVbut also cloud computing,
p53
aVwhich gives anyone in this theater
p54
aVthe kind of computational power
p55
aVwhich a few years ago was only the domain
p56
aVof three-letter agencies;
p57
aVand ubiquitous computing,
p58
aVwhich allows my phone, which is not a supercomputer,
p59
aVto connect to the Internet
p60
aVand do there hundreds of thousands
p61
aVof face metrics in a few seconds?
p62
aVWell, we conjecture that the result
p63
aVof this combination of technologies
p64
aVwill be a radical change in our very notions
p65
aVof privacy and anonymity.
p66
aa(lp67
VTo test that, we did an experiment
p68
aVon Carnegie Mellon University campus.
p69
aVWe asked students who were walking by
p70
aVto participate in a study,
p71
aVand we took a shot with a webcam,
p72
aVand we asked them to fill out a survey on a laptop.
p73
aVWhile they were filling out the survey,
p74
aVwe uploaded their shot to a cloud-computing cluster,
p75
aVand we started using a facial recognizer
p76
aVto match that shot to a database
p77
aVof some hundreds of thousands of images
p78
aVwhich we had downloaded from Facebook profiles.
p79
aVBy the time the subject reached the last page
p80
aVon the survey, the page had been dynamically updated
p81
aVwith the 10 best matching photos
p82
aVwhich the recognizer had found,
p83
aVand we asked the subjects to indicate
p84
aVwhether he or she found themselves in the photo.
p85
aa(lp86
VDo you see the subject?
p87
aVWell, the computer did, and in fact did so
p88
aVfor one out of three subjects.
p89
aa(lp90
VSo essentially, we can start from an anonymous face,
p91
aVoffline or online, and we can use facial recognition
p92
aVto give a name to that anonymous face
p93
aVthanks to social media data.
p94
aVBut a few years back, we did something else.
p95
aVWe started from social media data,
p96
aVwe combined it statistically with data
p97
aVfrom U.S. government social security,
p98
aVand we ended up predicting social security numbers,
p99
aVwhich in the United States
p100
aVare extremely sensitive information.
p101
aa(lp102
VDo you see where I'm going with this?
p103
aVSo if you combine the two studies together,
p104
aVthen the question becomes,
p105
aVcan you start from a face and,
p106
aVusing facial recognition, find a name
p107
aVand publicly available information
p108
aVabout that name and that person,
p109
aVand from that publicly available information
p110
aVinfer non-publicly available information,
p111
aVmuch more sensitive ones
p112
aVwhich you link back to the face?
p113
aVAnd the answer is, yes, we can, and we did.
p114
aVOf course, the accuracy keeps getting worse.
p115
aV[27% of subjects' first 5 SSN digits identified (with 4 attempts)]
p116
aVBut in fact, we even decided to develop an iPhone app
p117
aVwhich uses the phone's internal camera
p118
aVto take a shot of a subject
p119
aVand then upload it to a cloud
p120
aVand then do what I just described to you in real time:
p121
aVlooking for a match, finding public information,
p122
aVtrying to infer sensitive information,
p123
aVand then sending back to the phone
p124
aVso that it is overlaid on the face of the subject,
p125
aVan example of augmented reality,
p126
aVprobably a creepy example of augmented reality.
p127
aVIn fact, we didn't develop the app to make it available,
p128
aVjust as a proof of concept.
p129
aa(lp130
VIn fact, take these technologies
p131
aVand push them to their logical extreme.
p132
aVImagine a future in which strangers around you
p133
aVwill look at you through their Google Glasses
p134
aVor, one day, their contact lenses,
p135
aVand use seven or eight data points about you
p136
aVto infer anything else
p137
aVwhich may be known about you.
p138
aVWhat will this future without secrets look like?
p139
aVAnd should we care?
p140
aa(lp141
VWe may like to believe
p142
aVthat the future with so much wealth of data
p143
aVwould be a future with no more biases,
p144
aVbut in fact, having so much information
p145
aVdoesn't mean that we will make decisions
p146
aVwhich are more objective.
p147
aVIn another experiment, we presented to our subjects
p148
aVinformation about a potential job candidate.
p149
aVWe included in this information some references
p150
aVto some funny, absolutely legal,
p151
aVbut perhaps slightly embarrassing information
p152
aVthat the subject had posted online.
p153
aVNow interestingly, among our subjects,
p154
aVsome had posted comparable information,
p155
aVand some had not.
p156
aVWhich group do you think
p157
aVwas more likely to judge harshly our subject?
p158
aVParadoxically, it was the group
p159
aVwho had posted similar information,
p160
aVan example of moral dissonance.
p161
aa(lp162
VNow you may be thinking,
p163
aVthis does not apply to me,
p164
aVbecause I have nothing to hide.
p165
aVBut in fact, privacy is not about
p166
aVhaving something negative to hide.
p167
aVImagine that you are the H.R. director
p168
aVof a certain organization, and you receive résumés,
p169
aVand you decide to find more information about the candidates.
p170
aVTherefore, you Google their names
p171
aVand in a certain universe,
p172
aVyou find this information.
p173
aVOr in a parallel universe, you find this information.
p174
aVDo you think that you would be equally likely
p175
aVto call either candidate for an interview?
p176
aVIf you think so, then you are not
p177
aVlike the U.S. employers who are, in fact,
p178
aVpart of our experiment, meaning we did exactly that.
p179
aVWe created Facebook profiles, manipulating traits,
p180
aVthen we started sending out résumés to companies in the U.S.,
p181
aVand we detected, we monitored,
p182
aVwhether they were searching for our candidates,
p183
aVand whether they were acting on the information
p184
aVthey found on social media. And they were.
p185
aVDiscrimination was happening through social media
p186
aVfor equally skilled candidates.
p187
aa(lp188
VNow marketers like us to believe
p189
aVthat all information about us will always
p190
aVbe used in a manner which is in our favor.
p191
aVBut think again. Why should that be always the case?
p192
aVIn a movie which came out a few years ago,
p193
aV"Minority Report," a famous scene
p194
aVhad Tom Cruise walk in a mall
p195
aVand holographic personalized advertising
p196
aVwould appear around him.
p197
aVNow, that movie is set in 2054,
p198
aVabout 40 years from now,
p199
aVand as exciting as that technology looks,
p200
aVit already vastly underestimates
p201
aVthe amount of information that organizations
p202
aVcan gather about you, and how they can use it
p203
aVto influence you in a way that you will not even detect.
p204
aa(lp205
VSo as an example, this is another experiment
p206
aVactually we are running, not yet completed.
p207
aVImagine that an organization has access
p208
aVto your list of Facebook friends,
p209
aVand through some kind of algorithm
p210
aVthey can detect the two friends that you like the most.
p211
aVAnd then they create, in real time,
p212
aVa facial composite of these two friends.
p213
aVNow studies prior to ours have shown that people
p214
aVdon't recognize any longer even themselves
p215
aVin facial composites, but they react
p216
aVto those composites in a positive manner.
p217
aVSo next time you are looking for a certain product,
p218
aVand there is an ad suggesting you to buy it,
p219
aVit will not be just a standard spokesperson.
p220
aVIt will be one of your friends,
p221
aVand you will not even know that this is happening.
p222
aa(lp223
VNow the problem is that
p224
aVthe current policy mechanisms we have
p225
aVto protect ourselves from the abuses of personal information
p226
aVare like bringing a knife to a gunfight.
p227
aVOne of these mechanisms is transparency,
p228
aVtelling people what you are going to do with their data.
p229
aVAnd in principle, that's a very good thing.
p230
aVIt's necessary, but it is not sufficient.
p231
aVTransparency can be misdirected.
p232
aVYou can tell people what you are going to do,
p233
aVand then you still nudge them to disclose
p234
aVarbitrary amounts of personal information.
p235
aa(lp236
VSo in yet another experiment, this one with students,
p237
aVwe asked them to provide information
p238
aVabout their campus behavior,
p239
aVincluding pretty sensitive questions, such as this one.
p240
aV[Have you ever cheated in an exam?]
p241
aVNow to one group of subjects, we told them,
p242
aV"Only other students will see your answers."
p243
aVTo another group of subjects, we told them,
p244
aV"Students and faculty will see your answers."
p245
aVTransparency. Notification. And sure enough, this worked,
p246
aVin the sense that the first group of subjects
p247
aVwere much more likely to disclose than the second.
p248
aVIt makes sense, right?
p249
aVBut then we added the misdirection.
p250
aVWe repeated the experiment with the same two groups,
p251
aVthis time adding a delay
p252
aVbetween the time we told subjects
p253
aVhow we would use their data
p254
aVand the time we actually started answering the questions.
p255
aa(lp256
VHow long a delay do you think we had to add
p257
aVin order to nullify the inhibitory effect
p258
aVof knowing that faculty would see your answers?
p259
aVTen minutes?
p260
aVFive minutes?
p261
aVOne minute?
p262
aVHow about 15 seconds?
p263
aVFifteen seconds were sufficient to have the two groups
p264
aVdisclose the same amount of information,
p265
aVas if the second group now no longer cares
p266
aVfor faculty reading their answers.
p267
aa(lp268
VNow I have to admit that this talk so far
p269
aVmay sound exceedingly gloomy,
p270
aVbut that is not my point.
p271
aVIn fact, I want to share with you the fact that
p272
aVthere are alternatives.
p273
aVThe way we are doing things now is not the only way
p274
aVthey can done, and certainly not the best way
p275
aVthey can be done.
p276
aVWhen someone tells you, "People don't care about privacy,"
p277
aVconsider whether the game has been designed
p278
aVand rigged so that they cannot care about privacy,
p279
aVand coming to the realization that these manipulations occur
p280
aVis already halfway through the process
p281
aVof being able to protect yourself.
p282
aVWhen someone tells you that privacy is incompatible
p283
aVwith the benefits of big data,
p284
aVconsider that in the last 20 years,
p285
aVresearchers have created technologies
p286
aVto allow virtually any electronic transactions
p287
aVto take place in a more privacy-preserving manner.
p288
aVWe can browse the Internet anonymously.
p289
aVWe can send emails that can only be read
p290
aVby the intended recipient, not even the NSA.
p291
aVWe can have even privacy-preserving data mining.
p292
aVIn other words, we can have the benefits of big data
p293
aVwhile protecting privacy.
p294
aVOf course, these technologies imply a shifting
p295
aVof cost and revenues
p296
aVbetween data holders and data subjects,
p297
aVwhich is why, perhaps, you don't hear more about them.
p298
aa(lp299
VWhich brings me back to the Garden of Eden.
p300
aVThere is a second privacy interpretation
p301
aVof the story of the Garden of Eden
p302
aVwhich doesn't have to do with the issue
p303
aVof Adam and Eve feeling naked
p304
aVand feeling ashamed.
p305
aVYou can find echoes of this interpretation
p306
aVin John Milton's "Paradise Lost."
p307
aVIn the garden, Adam and Eve are materially content.
p308
aVThey're happy. They are satisfied.
p309
aVHowever, they also lack knowledge
p310
aVand self-awareness.
p311
aVThe moment they eat the aptly named
p312
aVfruit of knowledge,
p313
aVthat's when they discover themselves.
p314
aVThey become aware. They achieve autonomy.
p315
aVThe price to pay, however, is leaving the garden.
p316
aVSo privacy, in a way, is both the means
p317
aVand the price to pay for freedom.
p318
aa(lp319
VAgain, marketers tell us
p320
aVthat big data and social media
p321
aVare not just a paradise of profit for them,
p322
aVbut a Garden of Eden for the rest of us.
p323
aVWe get free content.
p324
aVWe get to play Angry Birds. We get targeted apps.
p325
aVBut in fact, in a few years, organizations
p326
aVwill know so much about us,
p327
aVthey will be able to infer our desires
p328
aVbefore we even form them, and perhaps
p329
aVbuy products on our behalf
p330
aVbefore we even know we need them.
p331
aa(lp332
VNow there was one English author
p333
aVwho anticipated this kind of future
p334
aVwhere we would trade away
p335
aVour autonomy and freedom for comfort.
p336
aVEven more so than George Orwell,
p337
aVthe author is, of course, Aldous Huxley.
p338
aVIn "Brave New World," he imagines a society
p339
aVwhere technologies that we created
p340
aVoriginally for freedom
p341
aVend up coercing us.
p342
aVHowever, in the book, he also offers us a way out
p343
aVof that society, similar to the path
p344
aVthat Adam and Eve had to follow to leave the garden.
p345
aVIn the words of the Savage,
p346
aVregaining autonomy and freedom is possible,
p347
aValthough the price to pay is steep.
p348
aVSo I do believe that one of the defining fights
p349
aVof our times will be the fight
p350
aVfor the control over personal information,
p351
aVthe fight over whether big data will become a force
p352
aVfor freedom,
p353
aVrather than a force which will hiddenly manipulate us.
p354
aa(lp355
VRight now, many of us
p356
aVdo not even know that the fight is going on,
p357
aVbut it is, whether you like it or not.
p358
aVAnd at the risk of playing the serpent,
p359
aVI will tell you that the tools for the fight
p360
aVare here, the awareness of what is going on,
p361
aVand in your hands,
p362
aVjust a few clicks away.
p363
aa(lp364
VThank you.
p365
aa(lp366
V(Applause)
p367
aasS'id'
p368
I1848
sS'title'
p369
VWhat will a future without secrets look like?
p370
s.