(dp0
S'transcript'
p1
(lp2
(lp3
VWe are built out of very small stuff,
p4
aVand we are embedded in a very large cosmos,
p5
aVand the fact is that we are not very good at understanding reality
p6
aVat either of those scales,
p7
aVand that's because our brains
p8
aVhaven't evolved to understand  the world at that scale.
p9
aa(lp10
VInstead, we're trapped on this very thin slice of perception
p11
aVright in the middle.
p12
aVBut it gets strange, because even at  that slice of reality that we call home,
p13
aVwe're not seeing most of the action that's going on.
p14
aVSo take the colors of our world.
p15
aVThis is light waves, electromagnetic radiation that bounces off objects
p16
aVand it hits specialized receptors in the back of our eyes.
p17
aVBut we're not seeing all the waves out there.
p18
aVIn fact, what we see
p19
aVis less than a 10 trillionth of what's out there.
p20
aVSo you have radio waves and microwaves
p21
aVand X-rays and gamma rays passing through your body right now
p22
aVand you're completely unaware of it,
p23
aVbecause you don't come with the proper biological receptors
p24
aVfor picking it up.
p25
aVThere are thousands of cell phone conversations
p26
aVpassing through you right now,
p27
aVand you're utterly blind to it.
p28
aa(lp29
VNow, it's not that these things are inherently unseeable.
p30
aVSnakes include some infrared in their reality,
p31
aVand honeybees include ultraviolet in their view of the world,
p32
aVand of course we build machines in the dashboards of our cars
p33
aVto pick up on signals in the radio frequency range,
p34
aVand we built machines in hospitals to pick up on the X-ray range.
p35
aVBut you can't sense any of those by yourself,
p36
aVat least not yet,
p37
aVbecause you don't come equipped with the proper sensors.
p38
aa(lp39
VNow, what this means is that our experience of reality
p40
aVis constrained by our biology,
p41
aVand that goes against the common sense notion
p42
aVthat our eyes and our ears and our fingertips
p43
aVare just picking up the objective reality that's out there.
p44
aVInstead, our brains are sampling just a little bit of the world.
p45
aa(lp46
VNow, across the animal kingdom,
p47
aVdifferent animals pick up on different parts of reality.
p48
aVSo in the blind and deaf world of the tick,
p49
aVthe important signals are temperature and butyric acid;
p50
aVin the world of the black ghost knifefish,
p51
aVits sensory world is lavishly colored by electrical fields;
p52
aVand for the echolocating bat,
p53
aVits reality is constructed out of air compression waves.
p54
aVThat's the slice of their ecosystem that they can pick up on,
p55
aVand we have a word for this in science.
p56
aVIt's called the umwelt,
p57
aVwhich is the German word for the surrounding world.
p58
aVNow, presumably, every animal assumes
p59
aVthat its umwelt is the entire objective reality out there,
p60
aVbecause why would you ever stop to imagine
p61
aVthat there's something beyond what we can sense.
p62
aVInstead, what we all do is we accept reality
p63
aVas it's presented to us.
p64
aa(lp65
VLet's do a consciousness-raiser on this.
p66
aVImagine that you are a bloodhound dog.
p67
aVYour whole world is about smelling.
p68
aVYou've got a long snout that has 200 million scent receptors in it,
p69
aVand you have wet nostrils that attract and trap scent molecules,
p70
aVand your nostrils even have slits so you can take big nosefuls of air.
p71
aVEverything is about smell for you.
p72
aVSo one day, you stop in your tracks with a revelation.
p73
aVYou look at your human owner and you think,
p74
aV"What is it like to have the pitiful, impoverished nose of a human?
p75
aV(Laughter)
p76
aVWhat is it like when you take a feeble little noseful of air?
p77
aVHow can you not know that there's a cat 100 yards away,
p78
aVor that your neighbor was on this very spot six hours ago?"
p79
aV(Laughter)
p80
aa(lp81
VSo because we're humans,
p82
aVwe've never experienced that world of smell,
p83
aVso we don't miss it,
p84
aVbecause we are firmly settled into our umwelt.
p85
aVBut the question is, do we have to be stuck there?
p86
aVSo as a neuroscientist, I'm interested in the way that technology
p87
aVmight expand our umwelt,
p88
aVand how that's going to change the experience of being human.
p89
aa(lp90
VSo we already know that we can marry our technology to our biology,
p91
aVbecause there are hundreds of thousands of people walking around
p92
aVwith artificial hearing and artificial vision.
p93
aVSo the way this works is, you take a microphone and you digitize the signal,
p94
aVand you put an electrode strip directly into the inner ear.
p95
aVOr, with the retinal implant, you take a camera
p96
aVand you digitize the signal, and then you plug an electrode grid
p97
aVdirectly into the optic nerve.
p98
aVAnd as recently as 15 years ago,
p99
aVthere were a lot of scientists who thought these technologies wouldn't work.
p100
aVWhy? It's because these technologies speak the language of Silicon Valley,
p101
aVand it's not exactly the same dialect as our natural biological sense organs.
p102
aVBut the fact is that it works;
p103
aVthe brain figures out how to use the signals just fine.
p104
aa(lp105
VNow, how do we understand that?
p106
aVWell, here's the big secret:
p107
aVYour brain is not hearing or seeing any of this.
p108
aVYour brain is locked in a vault of silence and darkness inside your skull.
p109
aVAll it ever sees are electrochemical signals
p110
aVthat come in along different data cables,
p111
aVand this is all it has to work with, and nothing more.
p112
aVNow, amazingly,
p113
aVthe brain is really good at taking in these signals
p114
aVand extracting patterns and assigning meaning,
p115
aVso that it takes this inner cosmos and puts together a story
p116
aVof this, your subjective world.
p117
aa(lp118
VBut here's the key point:
p119
aVYour brain doesn't know, and it doesn't care,
p120
aVwhere it gets the data from.
p121
aVWhatever information comes in, it just figures out what to do with it.
p122
aVAnd this is a very efficient kind of machine.
p123
aVIt's essentially a general purpose computing device,
p124
aVand it just takes in everything
p125
aVand figures out what it's going to do with it,
p126
aVand that, I think, frees up Mother Nature
p127
aVto tinker around with different sorts of input channels.
p128
aa(lp129
VSo I call this the P.H.  model of evolution,
p130
aVand I don't want to get too technical here,
p131
aVbut P.H. stands for Potato Head,
p132
aVand I use this name to emphasize that all these sensors
p133
aVthat we know and love, like our eyes and our ears and our fingertips,
p134
aVthese are merely peripheral plug-and-play devices:
p135
aVYou stick them in, and you're good to go.
p136
aVThe brain figures out what to do with the data that comes in.
p137
aVAnd when you look across the animal kingdom,
p138
aVyou find lots of peripheral devices.
p139
aVSo snakes have heat pits with which to detect infrared,
p140
aVand the ghost knifefish has electroreceptors,
p141
aVand the star-nosed mole has this appendage
p142
aVwith 22 fingers on it
p143
aVwith which it feels around and constructs a 3D model of the world,
p144
aVand many birds have magnetite so they can orient
p145
aVto the magnetic field of the planet.
p146
aVSo what this means is that nature doesn't have to continually
p147
aVredesign the brain.
p148
aVInstead, with the principles of brain operation established,
p149
aVall nature has to worry about is designing new peripherals.
p150
aa(lp151
VOkay. So what this means is this:
p152
aVThe lesson that surfaces
p153
aVis that there's nothing really special or fundamental
p154
aVabout the biology that we come to the table with.
p155
aVIt's just what we have inherited
p156
aVfrom a complex road of evolution.
p157
aVBut it's not what we have to stick with,
p158
aVand our best proof of principle of this
p159
aVcomes from what's called sensory substitution.
p160
aVAnd that refers to feeding information into the brain
p161
aVvia unusual sensory channels,
p162
aVand the brain just figures out what to do with it.
p163
aa(lp164
VNow, that might sound speculative,
p165
aVbut the first paper demonstrating this was published in the journal Nature in 1969.
p166
aVSo a scientist named Paul Bach-y-Rita
p167
aVput blind people in a modified dental chair,
p168
aVand he set up a video feed,
p169
aVand he put something in front of the camera,
p170
aVand then you would feel that
p171
aVpoked into your back with a grid of solenoids.
p172
aVSo if you wiggle a coffee cup in front of the camera,
p173
aVyou're feeling that in your back,
p174
aVand amazingly, blind people got pretty good
p175
aVat being able to determine what was in front of the camera
p176
aVjust by feeling it in the small of their back.
p177
aVNow, there have been many modern incarnations of this.
p178
aVThe sonic glasses take a video feed right in front of you
p179
aVand turn that into a sonic landscape,
p180
aVso as things move around, and get closer and farther,
p181
aVit sounds like "Bzz, bzz, bzz."
p182
aVIt sounds like a cacophony,
p183
aVbut after several weeks, blind people start getting pretty good
p184
aVat understanding what's in front of them
p185
aVjust based on what they're hearing.
p186
aVAnd it doesn't have to be through the ears:
p187
aVthis system uses an electrotactile grid on the forehead,
p188
aVso whatever's in front of the video feed, you're feeling it on your forehead.
p189
aVWhy the forehead? Because you're not using it for much else.
p190
aa(lp191
VThe most modern incarnation is called the brainport,
p192
aVand this is a little electrogrid that sits on your tongue,
p193
aVand the video feed gets turned into these little electrotactile signals,
p194
aVand blind people get so good at using this that they can throw a ball into a basket,
p195
aVor they can navigate complex obstacle courses.
p196
aVThey can come to see through their tongue.
p197
aVNow, that sounds completely insane, right?
p198
aVBut remember, all vision ever is
p199
aVis electrochemical signals coursing around in your brain.
p200
aVYour brain doesn't know where the signals come from.
p201
aVIt just figures out what to do with them.
p202
aa(lp203
VSo my interest in my lab is sensory substitution for the deaf,
p204
aVand this is a project I've undertaken
p205
aVwith a graduate student in my lab, Scott Novich,
p206
aVwho is spearheading this for his thesis.
p207
aVAnd here is what we wanted to do:
p208
aVwe wanted to make it so that sound from the world gets converted
p209
aVin some way so that a deaf person can understand what is being said.
p210
aVAnd we wanted to do this, given the power and ubiquity of portable computing,
p211
aVwe wanted to make sure that this would run on cell phones and tablets,
p212
aVand also we wanted to make this a wearable,
p213
aVsomething that you could wear under your clothing.
p214
aVSo here's the concept.
p215
aVSo as I'm speaking, my sound is getting captured by the tablet,
p216
aVand then it's getting mapped onto a vest that's covered in vibratory motors,
p217
aVjust like the motors in your cell phone.
p218
aVSo as I'm speaking,
p219
aVthe sound is getting translated to a pattern of vibration on the vest.
p220
aVNow, this is not just conceptual:
p221
aVthis tablet is transmitting Bluetooth, and I'm wearing the vest right now.
p222
aVSo as I'm speaking \u2014 (Applause) \u2014
p223
aVthe sound is getting translated into dynamic patterns of vibration.
p224
aVI'm feeling the sonic world around me.
p225
aa(lp226
VSo, we've been testing this with deaf people now,
p227
aVand it turns out that after just a little bit of time,
p228
aVpeople can start feeling, they can start understanding
p229
aVthe language of the vest.
p230
aa(lp231
VSo this is Jonathan. He's 37 years old. He has a master's degree.
p232
aVHe was born profoundly deaf,
p233
aVwhich means that there's a part of his umwelt that's unavailable to him.
p234
aVSo we had Jonathan train with the vest for four days, two hours a day,
p235
aVand here he is on the fifth day.
p236
aa(lp237
VScott Novich: You.
p238
aa(lp239
VDavid Eagleman: So Scott says a word, Jonathan feels it on the vest,
p240
aVand he writes it on the board.
p241
aa(lp242
VSN: Where. Where.
p243
aa(lp244
VDE: Jonathan is able to translate this complicated pattern of vibrations
p245
aVinto an understanding of what's being said.
p246
aa(lp247
VSN: Touch. Touch.
p248
aa(lp249
VDE: Now, he's not doing this \u2014
p250
aV(Applause) \u2014
p251
aVJonathan is not doing this consciously, because the patterns are too complicated,
p252
aVbut his brain is starting to unlock the pattern that allows it to figure out
p253
aVwhat the data mean,
p254
aVand our expectation is that, after wearing this for about three months,
p255
aVhe will have a direct perceptual experience of hearing
p256
aVin the same way that when a blind person passes a finger over braille,
p257
aVthe meaning comes directly off the page without any conscious intervention at all.
p258
aVNow, this technology has the potential to be a game-changer,
p259
aVbecause the only other solution for deafness is a cochlear implant,
p260
aVand that requires an invasive surgery.
p261
aVAnd this can be built for 40 times cheaper than a cochlear implant,
p262
aVwhich opens up this technology globally, even for the poorest countries.
p263
aa(lp264
VNow, we've been very encouraged by our results with sensory substitution,
p265
aVbut what we've been thinking a lot about is sensory addition.
p266
aVHow could we use a technology like this to add a completely new kind of sense,
p267
aVto expand the human umvelt?
p268
aVFor example, could we feed real-time data from the Internet
p269
aVdirectly into somebody's brain,
p270
aVand can they develop a direct perceptual experience?
p271
aa(lp272
VSo here's an experiment we're doing in the lab.
p273
aVA subject is feeling a real-time streaming feed from the Net of data
p274
aVfor five seconds.
p275
aVThen, two buttons appear, and he has to make a choice.
p276
aVHe doesn't know what's going on.
p277
aVHe makes a choice, and he gets feedback after one second.
p278
aVNow, here's the thing:
p279
aVThe subject has no idea what all the patterns mean,
p280
aVbut we're seeing if he gets better at figuring out which button to press.
p281
aVHe doesn't know that what we're feeding
p282
aVis real-time data from the stock market,
p283
aVand he's making buy and sell decisions.
p284
aV(Laughter)
p285
aVAnd the feedback is telling him whether he did the right thing or not.
p286
aVAnd what we're seeing is, can we expand the human umvelt
p287
aVso that he comes to have, after several weeks,
p288
aVa direct perceptual experience of the economic movements of the planet.
p289
aVSo we'll report on that later to see how well this goes.
p290
aV(Laughter)
p291
aa(lp292
VHere's another thing we're doing:
p293
aVDuring the talks this morning, we've been automatically scraping Twitter
p294
aVfor the TED2015 hashtag,
p295
aVand we've been doing an automated sentiment analysis,
p296
aVwhich means, are people using positive words or negative words or neutral?
p297
aVAnd while this has been going on,
p298
aVI have been feeling this,
p299
aVand so I am plugged in to the aggregate emotion
p300
aVof thousands of people in real time,
p301
aVand that's a new kind of human experience, because now I can know
p302
aVhow everyone's doing and how much you're loving this.
p303
aV(Laughter) (Applause)
p304
aVIt's a bigger experience than a human can normally have.
p305
aa(lp306
VWe're also expanding the umvelt of pilots.
p307
aVSo in this case, the vest is streaming nine different measures
p308
aVfrom this quadcopter,
p309
aVso pitch and yaw and roll and orientation and heading,
p310
aVand that improves this pilot's ability to fly it.
p311
aVIt's essentially like he's extending his skin up there, far away.
p312
aa(lp313
VAnd that's just the beginning.
p314
aVWhat we're envisioning is taking a modern cockpit full of gauges
p315
aVand instead of trying to read the whole thing, you feel it.
p316
aVWe live in a world of information now,
p317
aVand there is a difference between accessing big data
p318
aVand experiencing it.
p319
aa(lp320
VSo I think there's really no end to the possibilities
p321
aVon the horizon for human expansion.
p322
aVJust imagine an astronaut being able to feel
p323
aVthe overall health of the International Space Station,
p324
aVor, for that matter, having you feel the invisible states of your own health,
p325
aVlike your blood sugar and the state of your microbiome,
p326
aVor having 360-degree vision or seeing in infrared or ultraviolet.
p327
aa(lp328
VSo the key is this: As we move into the future,
p329
aVwe're going to increasingly be able to choose our own peripheral devices.
p330
aVWe no longer have to wait for Mother Nature's sensory gifts
p331
aVon her timescales,
p332
aVbut instead, like any good parent, she's given us the tools that we need
p333
aVto go out and define our own trajectory.
p334
aVSo the question now is,
p335
aVhow do you want to go out and experience your universe?
p336
aa(lp337
VThank you.
p338
aa(lp339
V(Applause)
p340
aa(lp341
VChris Anderson: Can you feel it? DE: Yeah.
p342
aa(lp343
VActually, this was the first time I felt applause on the vest.
p344
aVIt's nice. It's like a massage. (Laughter)
p345
aa(lp346
VCA: Twitter's going crazy. Twitter's going mad.
p347
aVSo that stock market experiment.
p348
aVThis could be the first experiment that secures its funding forevermore,
p349
aVright, if successful?
p350
aa(lp351
VDE: Well, that's right, I wouldn't have to write to NIH anymore.
p352
aa(lp353
VCA: Well look, just to be skeptical for a minute,
p354
aVI mean, this is amazing, but isn't most of the evidence so far
p355
aVthat sensory substitution works,
p356
aVnot necessarily  that sensory addition works?
p357
aVI mean, isn't it possible that the blind person can see through their tongue
p358
aVbecause the visual cortex is still there, ready to process,
p359
aVand that that is needed as part of it?
p360
aa(lp361
VDE: That's a great question. We actually have no idea
p362
aVwhat the theoretical limits are of what kind of data the brain can take in.
p363
aVThe general story, though, is that it's extraordinarily flexible.
p364
aVSo when a person goes blind, what we used to call their visual cortex
p365
aVgets taken over by other things, by touch, by hearing, by vocabulary.
p366
aVSo what that tells us is that the cortex is kind of a one-trick pony.
p367
aVIt just runs certain kinds of computations on things.
p368
aVAnd when we look around at things like braille, for example,
p369
aVpeople are getting information through bumps on their fingers.
p370
aVSo I don't think we have any reason to think there's a theoretical limit
p371
aVthat we know the edge of.
p372
aa(lp373
VCA: If this checks out, you're going to be deluged.
p374
aVThere are so many possible applications for this.
p375
aVAre you ready for this? What are you most excited about, the direction it might go?
p376
aVDE: I mean, I think there's a lot of applications here.
p377
aVIn terms of beyond sensory substitution, the things I started mentioning
p378
aVabout astronauts on the space station, they spend a lot of their time
p379
aVmonitoring things, and they could instead just get what's going on,
p380
aVbecause what this is really good for is multidimensional data.
p381
aVThe key is this: Our visual systems are good at detecting blobs and edges,
p382
aVbut they're really bad at what our world has become,
p383
aVwhich is screens with lots and lots of data.
p384
aVWe have to crawl that with our attentional systems.
p385
aVSo this is a way of just feeling the state of something,
p386
aVjust like the way you know the state of your body as you're standing around.
p387
aVSo I think heavy machinery, safety, feeling the state of a factory,
p388
aVof your equipment, that's one place it'll go right away.
p389
aa(lp390
VCA: David Eagleman, that was one mind-blowing talk. Thank you very much.
p391
aa(lp392
VDE: Thank you, Chris. (Applause)
p393
aasS'id'
p394
I2215
sS'title'
p395
VCan we create new senses for humans?
p396
s.