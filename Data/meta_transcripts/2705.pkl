(dp0
S'transcript'
p1
(lp2
(lp3
VHello, I'm Joy, a poet of code,
p4
aVon a mission to stop an unseen force that's rising,
p5
aVa force that I called "the coded gaze,"
p6
aVmy term for algorithmic bias.
p7
aa(lp8
VAlgorithmic bias, like human bias, results in unfairness.
p9
aVHowever, algorithms, like viruses, can spread bias on a massive scale
p10
aVat a rapid pace.
p11
aVAlgorithmic bias can also lead to exclusionary experiences
p12
aVand discriminatory practices.
p13
aVLet me show you what I mean.
p14
aa(lp15
V(Video) Joy Buolamwini: Hi, camera. I've got a face.
p16
aVCan you see my face?
p17
aVNo-glasses face?
p18
aVYou can see her face.
p19
aVWhat about my face?
p20
aVI've got a mask. Can you see my mask?
p21
aa(lp22
VJoy Buolamwini: So how did this happen?
p23
aVWhy am I sitting in front of a computer
p24
aVin a white mask,
p25
aVtrying to be detected by a cheap webcam?
p26
aVWell, when I'm not fighting the coded gaze
p27
aVas a poet of code,
p28
aVI'm a graduate student at the MIT Media Lab,
p29
aVand there I have the opportunity to work on all sorts of whimsical projects,
p30
aVincluding the Aspire Mirror,
p31
aVa project I did so I could project digital masks onto my reflection.
p32
aVSo in the morning, if I wanted to feel powerful,
p33
aVI could put on a lion.
p34
aVIf I wanted to be uplifted, I might have a quote.
p35
aVSo I used generic facial recognition software
p36
aVto build the system,
p37
aVbut found it was really hard to test it unless I wore a white mask.
p38
aa(lp39
VUnfortunately, I've run into this issue before.
p40
aVWhen I was an undergraduate at Georgia Tech studying computer science,
p41
aVI used to work on social robots,
p42
aVand one of my tasks was to get a robot to play peek-a-boo,
p43
aVa simple turn-taking game
p44
aVwhere partners cover their face and then uncover it saying, "Peek-a-boo!"
p45
aVThe problem is, peek-a-boo doesn't really work if I can't see you,
p46
aVand my robot couldn't see me.
p47
aVBut I borrowed my roommate's face to get the project done,
p48
aVsubmitted the assignment,
p49
aVand figured, you know what, somebody else will solve this problem.
p50
aa(lp51
VNot too long after,
p52
aVI was in Hong Kong for an entrepreneurship competition.
p53
aVThe organizers decided to take participants
p54
aVon a tour of local start-ups.
p55
aVOne of the start-ups had a social robot,
p56
aVand they decided to do a demo.
p57
aVThe demo worked on everybody until it got to me,
p58
aVand you can probably guess it.
p59
aVIt couldn't detect my face.
p60
aVI asked the developers what was going on,
p61
aVand it turned out we had used the same generic facial recognition software.
p62
aVHalfway around the world,
p63
aVI learned that algorithmic bias can travel as quickly
p64
aVas it takes to download some files off of the internet.
p65
aa(lp66
VSo what's going on? Why isn't my face being detected?
p67
aVWell, we have to look at how we give machines sight.
p68
aVComputer vision uses machine learning techniques
p69
aVto do facial recognition.
p70
aVSo how this works is, you create a training set with examples of faces.
p71
aVThis is a face. This is a face. This is not a face.
p72
aVAnd over time, you can teach a computer how to recognize other faces.
p73
aVHowever, if the training sets aren't really that diverse,
p74
aVany face that deviates too much from the established norm
p75
aVwill be harder to detect,
p76
aVwhich is what was happening to me.
p77
aa(lp78
VBut don't worry \u2014 there's some good news.
p79
aVTraining sets don't just materialize out of nowhere.
p80
aVWe actually can create them.
p81
aVSo there's an opportunity to create full-spectrum training sets
p82
aVthat reflect a richer portrait of humanity.
p83
aa(lp84
VNow you've seen in my examples
p85
aVhow social robots
p86
aVwas how I found out about exclusion with algorithmic bias.
p87
aVBut algorithmic bias can also lead to discriminatory practices.
p88
aVAcross the US,
p89
aVpolice departments are starting to use facial recognition software
p90
aVin their crime-fighting arsenal.
p91
aVGeorgetown Law published a report
p92
aVshowing that one in two adults in the US \u2014 that's 117 million people \u2014
p93
aVhave their faces in facial recognition networks.
p94
aVPolice departments can currently look at these networks unregulated,
p95
aVusing algorithms that have not been audited for accuracy.
p96
aVYet we know facial recognition is not fail proof,
p97
aVand labeling faces consistently remains a challenge.
p98
aVYou might have seen this on Facebook.
p99
aVMy friends and I laugh all the time when we see other people
p100
aVmislabeled in our photos.
p101
aVBut misidentifying a suspected criminal is no laughing matter,
p102
aVnor is breaching civil liberties.
p103
aa(lp104
VMachine learning is being used for facial recognition,
p105
aVbut it's also extending beyond the realm of computer vision.
p106
aVIn her book, "Weapons of Math Destruction,"
p107
aVdata scientist Cathy O'Neil talks about the rising new WMDs \u2014
p108
aVwidespread, mysterious and destructive algorithms
p109
aVthat are increasingly being used to make decisions
p110
aVthat impact more aspects of our lives.
p111
aVSo who gets hired or fired?
p112
aVDo you get that loan? Do you get insurance?
p113
aVAre you admitted into the college you wanted to get into?
p114
aVDo you and I pay the same price for the same product
p115
aVpurchased on the same platform?
p116
aa(lp117
VLaw enforcement is also starting to use machine learning
p118
aVfor predictive policing.
p119
aVSome judges use machine-generated risk scores to determine
p120
aVhow long an individual is going to spend in prison.
p121
aVSo we really have to think about these decisions.
p122
aVAre they fair?
p123
aVAnd we've seen that algorithmic bias
p124
aVdoesn't necessarily always lead to fair outcomes.
p125
aa(lp126
VSo what can we do about it?
p127
aVWell, we can start thinking about how we create more inclusive code
p128
aVand employ inclusive coding practices.
p129
aVIt really starts with people.
p130
aVSo who codes matters.
p131
aVAre we creating full-spectrum teams with diverse individuals
p132
aVwho can check each other's blind spots?
p133
aVOn the technical side, how we code matters.
p134
aVAre we factoring in fairness as we're developing systems?
p135
aVAnd finally, why we code matters.
p136
aVWe've used tools of computational creation to unlock immense wealth.
p137
aVWe now have the opportunity to unlock even greater equality
p138
aVif we make social change a priority
p139
aVand not an afterthought.
p140
aVAnd so these are the three tenets that will make up the "incoding" movement.
p141
aVWho codes matters,
p142
aVhow we code matters
p143
aVand why we code matters.
p144
aa(lp145
VSo to go towards incoding, we can start thinking about
p146
aVbuilding platforms that can identify bias
p147
aVby collecting people's experiences like the ones I shared,
p148
aVbut also auditing existing software.
p149
aVWe can also start to create more inclusive training sets.
p150
aVImagine a "Selfies for Inclusion" campaign
p151
aVwhere you and I can help developers test and create
p152
aVmore inclusive training sets.
p153
aVAnd we can also start thinking more conscientiously
p154
aVabout the social impact of the technology that we're developing.
p155
aa(lp156
VTo get the incoding movement started,
p157
aVI've launched the Algorithmic Justice League,
p158
aVwhere anyone who cares about fairness can help fight the coded gaze.
p159
aVOn codedgaze.com, you can report bias,
p160
aVrequest audits, become a tester
p161
aVand join the ongoing conversation,
p162
aV#codedgaze.
p163
aa(lp164
VSo I invite you to join me
p165
aVin creating a world where technology works for all of us,
p166
aVnot just some of us,
p167
aVa world where we value inclusion and center social change.
p168
aa(lp169
VThank you.
p170
aa(lp171
V(Applause)
p172
aa(lp173
VBut I have one question:
p174
aVWill you join me in the fight?
p175
aa(lp176
V(Laughter)
p177
aa(lp178
V(Applause)
p179
aasS'id'
p180
I2705
sS'title'
p181
VHow I'm fighting bias in algorithms
p182
s.