(dp0
S'transcript'
p1
(lp2
(lp3
VPower.
p4
aVThat is the word that comes to mind.
p5
aVWe're the new technologists.
p6
aVWe have a lot of data, so we have a lot of power.
p7
aVHow much power do we have?
p8
aVScene from a movie: "Apocalypse Now" \u2014 great movie.
p9
aVWe've got to get our hero, Captain Willard, to the mouth of the Nung River
p10
aVso he can go pursue Colonel Kurtz.
p11
aVThe way we're going to do this is fly him in and drop him off.
p12
aVSo the scene:
p13
aVthe sky is filled with this fleet of helicopters carrying him in.
p14
aVAnd there's this loud, thrilling music in the background,
p15
aVthis wild music.
p16
aV\u266b Dum da ta da dum \u266b
p17
aV\u266b Dum da ta da dum \u266b
p18
aV\u266b Da ta da da \u266b
p19
aVThat's a lot of power.
p20
aVThat's the kind of power I feel in this room.
p21
aVThat's the kind of power we have
p22
aVbecause of all of the data that we have.
p23
aa(lp24
VLet's take an example.
p25
aVWhat can we do
p26
aVwith just one person's data?
p27
aVWhat can we do
p28
aVwith that guy's data?
p29
aVI can look at your financial records.
p30
aVI can tell if you pay your bills on time.
p31
aVI know if you're good to give a loan to.
p32
aVI can look at your medical records; I can see if your pump is still pumping \u2014
p33
aVsee if you're good to offer insurance to.
p34
aVI can look at your clicking patterns.
p35
aVWhen you come to my website, I actually know what you're going to do already
p36
aVbecause I've seen you visit millions of websites before.
p37
aVAnd I'm sorry to tell you,
p38
aVyou're like a poker player, you have a tell.
p39
aVI can tell with data analysis what you're going to do
p40
aVbefore you even do it.
p41
aVI know what you like. I know who you are,
p42
aVand that's even before I look at your mail
p43
aVor your phone.
p44
aa(lp45
VThose are the kinds of things we can do
p46
aVwith the data that we have.
p47
aVBut I'm not actually here to talk about what we can do.
p48
aVI'm here to talk about what we should do.
p49
aVWhat's the right thing to do?
p50
aa(lp51
VNow I see some puzzled looks
p52
aVlike, "Why are you asking us what's the right thing to do?
p53
aVWe're just building this stuff. Somebody else is using it."
p54
aVFair enough.
p55
aVBut it brings me back.
p56
aVI think about World War II \u2014
p57
aVsome of our great technologists then,
p58
aVsome of our great physicists,
p59
aVstudying nuclear fission and fusion \u2014
p60
aVjust nuclear stuff.
p61
aVWe gather together these physicists in Los Alamos
p62
aVto see what they'll build.
p63
aVWe want the people building the technology
p64
aVthinking about what we should be doing with the technology.
p65
aa(lp66
VSo what should we be doing with that guy's data?
p67
aVShould we be collecting it, gathering it,
p68
aVso we can make his online experience better?
p69
aVSo we can make money?
p70
aVSo we can protect ourselves
p71
aVif he was up to no good?
p72
aVOr should we respect his privacy,
p73
aVprotect his dignity and leave him alone?
p74
aVWhich one is it?
p75
aVHow should we figure it out?
p76
aa(lp77
VI know: crowdsource. Let's crowdsource this.
p78
aVSo to get people warmed up,
p79
aVlet's start with an easy question \u2014
p80
aVsomething I'm sure everybody here has an opinion about:
p81
aViPhone versus Android.
p82
aVLet's do a show of hands \u2014 iPhone.
p83
aVUh huh.
p84
aVAndroid.
p85
aVYou'd think with a bunch of smart people
p86
aVwe wouldn't be such suckers just for the pretty phones.
p87
aV(Laughter)
p88
aVNext question,
p89
aVa little bit harder.
p90
aVShould we be collecting all of that guy's data
p91
aVto make his experiences better
p92
aVand to protect ourselves in case he's up to no good?
p93
aVOr should we leave him alone?
p94
aVCollect his data.
p95
aVLeave him alone.
p96
aVYou're safe. It's fine.
p97
aV(Laughter)
p98
aVOkay, last question \u2014
p99
aVharder question \u2014
p100
aVwhen trying to evaluate
p101
aVwhat we should do in this case,
p102
aVshould we use a Kantian deontological moral framework,
p103
aVor should we use a Millian consequentialist one?
p104
aVKant.
p105
aVMill.
p106
aVNot as many votes.
p107
aV(Laughter)
p108
aVYeah, that's a terrifying result.
p109
aVTerrifying, because we have stronger opinions
p110
aVabout our hand-held devices
p111
aVthan about the moral framework
p112
aVwe should use to guide our decisions.
p113
aa(lp114
VHow do we know what to do with all the power we have
p115
aVif we don't have a moral framework?
p116
aVWe know more about mobile operating systems,
p117
aVbut what we really need is a moral operating system.
p118
aVWhat's a moral operating system?
p119
aVWe all know right and wrong, right?
p120
aVYou feel good when you do something right,
p121
aVyou feel bad when you do something wrong.
p122
aVOur parents teach us that: praise with the good, scold with the bad.
p123
aVBut how do we figure out what's right and wrong?
p124
aVAnd from day to day, we have the techniques that we use.
p125
aVMaybe we just follow our gut.
p126
aVMaybe we take a vote \u2014 we crowdsource.
p127
aVOr maybe we punt \u2014
p128
aVask the legal department, see what they say.
p129
aVIn other words, it's kind of random,
p130
aVkind of ad hoc,
p131
aVhow we figure out what we should do.
p132
aVAnd maybe, if we want to be on surer footing,
p133
aVwhat we really want is a moral framework that will help guide us there,
p134
aVthat will tell us what kinds of things are right and wrong in the first place,
p135
aVand how would we know in a given situation what to do.
p136
aa(lp137
VSo let's get a moral framework.
p138
aVWe're numbers people, living by numbers.
p139
aVHow can we use numbers
p140
aVas the basis for a moral framework?
p141
aVI know a guy who did exactly that.
p142
aVA brilliant guy \u2014
p143
aVhe's been dead 2,500 years.
p144
aVPlato, that's right.
p145
aVRemember him \u2014 old philosopher?
p146
aVYou were sleeping during that class.
p147
aVAnd Plato, he had a lot of the same concerns that we did.
p148
aVHe was worried about right and wrong.
p149
aVHe wanted to know what is just.
p150
aVBut he was worried that all we seem to be doing
p151
aVis trading opinions about this.
p152
aVHe says something's just. She says something else is just.
p153
aVIt's kind of convincing when he talks and when she talks too.
p154
aVI'm just going back and forth; I'm not getting anywhere.
p155
aVI don't want opinions; I want knowledge.
p156
aVI want to know the truth about justice \u2014
p157
aVlike we have truths in math.
p158
aVIn math, we know the objective facts.
p159
aVTake a number, any number \u2014 two.
p160
aVFavorite number. I love that number.
p161
aVThere are truths about two.
p162
aVIf you've got two of something,
p163
aVyou add two more, you get four.
p164
aVThat's true no matter what thing you're talking about.
p165
aVIt's an objective truth about the form of two,
p166
aVthe abstract form.
p167
aVWhen you have two of anything \u2014 two eyes, two ears, two noses,
p168
aVjust two protrusions \u2014
p169
aVthose all partake of the form of two.
p170
aVThey all participate in the truths that two has.
p171
aVThey all have two-ness in them.
p172
aVAnd therefore, it's not a matter of opinion.
p173
aa(lp174
VWhat if, Plato thought,
p175
aVethics was like math?
p176
aVWhat if there were a pure form of justice?
p177
aVWhat if there are truths about justice,
p178
aVand you could just look around in this world
p179
aVand see which things participated,
p180
aVpartook of that form of justice?
p181
aVThen you would know what was really just and what wasn't.
p182
aVIt wouldn't be a matter
p183
aVof just opinion or just appearances.
p184
aVThat's a stunning vision.
p185
aVI mean, think about that. How grand. How ambitious.
p186
aVThat's as ambitious as we are.
p187
aVHe wants to solve ethics.
p188
aVHe wants objective truths.
p189
aVIf you think that way,
p190
aVyou have a Platonist moral framework.
p191
aa(lp192
VIf you don't think that way,
p193
aVwell, you have a lot of company in the history of Western philosophy,
p194
aVbecause the tidy idea, you know, people criticized it.
p195
aVAristotle, in particular, he was not amused.
p196
aVHe thought it was impractical.
p197
aVAristotle said, "We should seek only so much precision in each subject
p198
aVas that subject allows."
p199
aVAristotle thought ethics wasn't a lot like math.
p200
aVHe thought ethics was a matter of making decisions in the here-and-now
p201
aVusing our best judgment
p202
aVto find the right path.
p203
aVIf you think that, Plato's not your guy.
p204
aVBut don't give up.
p205
aVMaybe there's another way
p206
aVthat we can use numbers as the basis of our moral framework.
p207
aa(lp208
VHow about this:
p209
aVWhat if in any situation you could just calculate,
p210
aVlook at the choices,
p211
aVmeasure out which one's better and know what to do?
p212
aVThat sound familiar?
p213
aVThat's a utilitarian moral framework.
p214
aVJohn Stuart Mill was a great advocate of this \u2014
p215
aVnice guy besides \u2014
p216
aVand only been dead 200 years.
p217
aVSo basis of utilitarianism \u2014
p218
aVI'm sure you're familiar at least.
p219
aVThe three people who voted for Mill before are familiar with this.
p220
aVBut here's the way it works.
p221
aVWhat if morals, what if what makes something moral
p222
aVis just a matter of if it maximizes pleasure
p223
aVand minimizes pain?
p224
aVIt does something intrinsic to the act.
p225
aVIt's not like its relation to some abstract form.
p226
aVIt's just a matter of the consequences.
p227
aVYou just look at the consequences
p228
aVand see if, overall, it's for the good or for the worse.
p229
aVThat would be simple. Then we know what to do.
p230
aa(lp231
VLet's take an example.
p232
aVSuppose I go up
p233
aVand I say, "I'm going to take your phone."
p234
aVNot just because it rang earlier,
p235
aVbut I'm going to take it because I made a little calculation.
p236
aVI thought, that guy looks suspicious.
p237
aVAnd what if he's been sending little messages to Bin Laden's hideout \u2014
p238
aVor whoever took over after Bin Laden \u2014
p239
aVand he's actually like a terrorist, a sleeper cell.
p240
aVI'm going to find that out, and when I find that out,
p241
aVI'm going to prevent a huge amount of damage that he could cause.
p242
aVThat has a very high utility to prevent that damage.
p243
aVAnd compared to the little pain that it's going to cause \u2014
p244
aVbecause it's going to be embarrassing when I'm looking on his phone
p245
aVand seeing that he has a Farmville problem and that whole bit \u2014
p246
aVthat's overwhelmed
p247
aVby the value of looking at the phone.
p248
aVIf you feel that way,
p249
aVthat's a utilitarian choice.
p250
aa(lp251
VBut maybe you don't feel that way either.
p252
aVMaybe you think, it's his phone.
p253
aVIt's wrong to take his phone
p254
aVbecause he's a person
p255
aVand he has rights and he has dignity,
p256
aVand we can't just interfere with that.
p257
aVHe has autonomy.
p258
aVIt doesn't matter what the calculations are.
p259
aVThere are things that are intrinsically wrong \u2014
p260
aVlike lying is wrong,
p261
aVlike torturing innocent children is wrong.
p262
aVKant was very good on this point,
p263
aVand he said it a little better than I'll say it.
p264
aVHe said we should use our reason
p265
aVto figure out the rules by which we should guide our conduct,
p266
aVand then it is our duty to follow those rules.
p267
aVIt's not a matter of calculation.
p268
aa(lp269
VSo let's stop.
p270
aVWe're right in the thick of it, this philosophical thicket.
p271
aVAnd this goes on for thousands of years,
p272
aVbecause these are hard questions,
p273
aVand I've only got 15 minutes.
p274
aVSo let's cut to the chase.
p275
aVHow should we be making our decisions?
p276
aVIs it Plato, is it Aristotle, is it Kant, is it Mill?
p277
aVWhat should we be doing? What's the answer?
p278
aVWhat's the formula that we can use in any situation
p279
aVto determine what we should do,
p280
aVwhether we should use that guy's data or not?
p281
aVWhat's the formula?
p282
aVThere's not a formula.
p283
aVThere's not a simple answer.
p284
aa(lp285
VEthics is hard.
p286
aVEthics requires thinking.
p287
aVAnd that's uncomfortable.
p288
aVI know; I spent a lot of my career
p289
aVin artificial intelligence,
p290
aVtrying to build machines that could do some of this thinking for us,
p291
aVthat could give us answers.
p292
aVBut they can't.
p293
aVYou can't just take human thinking
p294
aVand put it into a machine.
p295
aVWe're the ones who have to do it.
p296
aVHappily, we're not machines, and we can do it.
p297
aVNot only can we think,
p298
aVwe must.
p299
aVHannah Arendt said,
p300
aV"The sad truth
p301
aVis that most evil done in this world
p302
aVis not done by people
p303
aVwho choose to be evil.
p304
aVIt arises from not thinking."
p305
aVThat's what she called the "banality of evil."
p306
aVAnd the response to that
p307
aVis that we demand the exercise of thinking
p308
aVfrom every sane person.
p309
aa(lp310
VSo let's do that. Let's think.
p311
aVIn fact, let's start right now.
p312
aVEvery person in this room do this:
p313
aVthink of the last time you had a decision to make
p314
aVwhere you were worried to do the right thing,
p315
aVwhere you wondered, "What should I be doing?"
p316
aVBring that to mind,
p317
aVand now reflect on that
p318
aVand say, "How did I come up that decision?
p319
aVWhat did I do? Did I follow my gut?
p320
aVDid I have somebody vote on it? Or did I punt to legal?"
p321
aVOr now we have a few more choices.
p322
aV"Did I evaluate what would be the highest pleasure
p323
aVlike Mill would?
p324
aVOr like Kant, did I use reason to figure out what was intrinsically right?"
p325
aVThink about it. Really bring it to mind. This is important.
p326
aVIt is so important
p327
aVwe are going to spend 30 seconds of valuable TEDTalk time
p328
aVdoing nothing but thinking about this.
p329
aVAre you ready? Go.
p330
aa(lp331
VStop. Good work.
p332
aVWhat you just did,
p333
aVthat's the first step towards taking responsibility
p334
aVfor what we should do with all of our power.
p335
aa(lp336
VNow the next step \u2014 try this.
p337
aVGo find a friend and explain to them
p338
aVhow you made that decision.
p339
aVNot right now. Wait till I finish talking.
p340
aVDo it over lunch.
p341
aVAnd don't just find another technologist friend;
p342
aVfind somebody different than you.
p343
aVFind an artist or a writer \u2014
p344
aVor, heaven forbid, find a philosopher and talk to them.
p345
aVIn fact, find somebody from the humanities.
p346
aVWhy? Because they think about problems
p347
aVdifferently than we do as technologists.
p348
aVJust a few days ago, right across the street from here,
p349
aVthere was hundreds of people gathered together.
p350
aVIt was technologists and humanists
p351
aVat that big BiblioTech Conference.
p352
aVAnd they gathered together
p353
aVbecause the technologists wanted to learn
p354
aVwhat it would be like to think from a humanities perspective.
p355
aVYou have someone from Google
p356
aVtalking to someone who does comparative literature.
p357
aVYou're thinking about the relevance of 17th century French theater \u2014
p358
aVhow does that bear upon venture capital?
p359
aVWell that's interesting. That's a different way of thinking.
p360
aVAnd when you think in that way,
p361
aVyou become more sensitive to the human considerations,
p362
aVwhich are crucial to making ethical decisions.
p363
aa(lp364
VSo imagine that right now
p365
aVyou went and you found your musician friend.
p366
aVAnd you're telling him what we're talking about,
p367
aVabout our whole data revolution and all this \u2014
p368
aVmaybe even hum a few bars of our theme music.
p369
aV\u266b Dum ta da da dum dum ta da da dum \u266b
p370
aVWell, your musician friend will stop you and say,
p371
aV"You know, the theme music
p372
aVfor your data revolution,
p373
aVthat's an opera, that's Wagner.
p374
aVIt's based on Norse legend.
p375
aVIt's Gods and mythical creatures
p376
aVfighting over magical jewelry."
p377
aVThat's interesting.
p378
aVNow it's also a beautiful opera,
p379
aVand we're moved by that opera.
p380
aVWe're moved because it's about the battle
p381
aVbetween good and evil,
p382
aVabout right and wrong.
p383
aVAnd we care about right and wrong.
p384
aVWe care what happens in that opera.
p385
aVWe care what happens in "Apocalypse Now."
p386
aVAnd we certainly care
p387
aVwhat happens with our technologies.
p388
aa(lp389
VWe have so much power today,
p390
aVit is up to us to figure out what to do,
p391
aVand that's the good news.
p392
aVWe're the ones writing this opera.
p393
aVThis is our movie.
p394
aVWe figure out what will happen with this technology.
p395
aVWe determine how this will all end.
p396
aa(lp397
VThank you.
p398
aa(lp399
V(Applause)
p400
aasS'id'
p401
I1162
sS'title'
p402
VWe need a "moral operating system"
p403
s.