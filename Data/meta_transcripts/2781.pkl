(dp0
S'transcript'
p1
(lp2
(lp3
VThis is Lee Sedol.
p4
aVLee Sedol is one of the world's greatest Go players,
p5
aVand he's having what my friends in Silicon Valley call
p6
aVa "Holy Cow" moment \u2014
p7
aa(lp8
V(Laughter)
p9
aa(lp10
Va moment where we realize
p11
aVthat AI is actually progressing a lot faster than we expected.
p12
aVSo humans have lost on the Go board. What about the real world?
p13
aa(lp14
VWell, the real world is much bigger,
p15
aVmuch more complicated than the Go board.
p16
aVIt's a lot less visible,
p17
aVbut it's still a decision problem.
p18
aVAnd if we think about some of the technologies
p19
aVthat are coming down the pike ...
p20
aVNoriko [Arai] mentioned that reading is not yet happening in machines,
p21
aVat least with understanding.
p22
aVBut that will happen,
p23
aVand when that happens,
p24
aVvery soon afterwards,
p25
aVmachines will have read everything that the human race has ever written.
p26
aVAnd that will enable machines,
p27
aValong with the ability to look further ahead than humans can,
p28
aVas we've already seen in Go,
p29
aVif they also have access to more information,
p30
aVthey'll be able to make better decisions in the real world than we can.
p31
aVSo is that a good thing?
p32
aVWell, I hope so.
p33
aa(lp34
VOur entire civilization, everything that we value,
p35
aVis based on our intelligence.
p36
aVAnd if we had access to a lot more intelligence,
p37
aVthen there's really no limit to what the human race can do.
p38
aVAnd I think this could be, as some people have described it,
p39
aVthe biggest event in human history.
p40
aVSo why are people saying things like this,
p41
aVthat AI might spell the end of the human race?
p42
aVIs this a new thing?
p43
aVIs it just Elon Musk and Bill Gates and Stephen Hawking?
p44
aa(lp45
VActually, no. This idea has been around for a while.
p46
aVHere's a quotation:
p47
aV"Even if we could keep the machines in a subservient position,
p48
aVfor instance, by turning off the power at strategic moments" \u2014
p49
aVand I'll come back to that "turning off the power" idea later on \u2014
p50
aV"we should, as a species, feel greatly humbled."
p51
aVSo who said this? This is Alan Turing in 1951.
p52
aVAlan Turing, as you know, is the father of computer science
p53
aVand in many ways, the father of AI as well.
p54
aVSo if we think about this problem,
p55
aVthe problem of creating something more intelligent than your own species,
p56
aVwe might call this "the gorilla problem,"
p57
aVbecause gorillas' ancestors did this a few million years ago,
p58
aVand now we can ask the gorillas:
p59
aVWas this a good idea?
p60
aa(lp61
VSo here they are having a meeting to discuss whether it was a good idea,
p62
aVand after a little while, they conclude, no,
p63
aVthis was a terrible idea.
p64
aVOur species is in dire straits.
p65
aVIn fact, you can see the existential sadness in their eyes.
p66
aa(lp67
V(Laughter)
p68
aa(lp69
VSo this queasy feeling that making something smarter than your own species
p70
aVis maybe not a good idea \u2014
p71
aVwhat can we do about that?
p72
aVWell, really nothing, except stop doing AI,
p73
aVand because of all the benefits that I mentioned
p74
aVand because I'm an AI researcher,
p75
aVI'm not having that.
p76
aVI actually want to be able to keep doing AI.
p77
aa(lp78
VSo we actually need to nail down the problem a bit more.
p79
aVWhat exactly is the problem?
p80
aVWhy is better AI possibly a catastrophe?
p81
aa(lp82
VSo here's another quotation:
p83
aV"We had better be quite sure that the purpose put into the machine
p84
aVis the purpose which we really desire."
p85
aVThis was said by Norbert Wiener in 1960,
p86
aVshortly after he watched one of the very early learning systems
p87
aVlearn to play checkers better than its creator.
p88
aVBut this could equally have been said
p89
aVby King Midas.
p90
aVKing Midas said, "I want everything I touch to turn to gold,"
p91
aVand he got exactly what he asked for.
p92
aVThat was the purpose that he put into the machine,
p93
aVso to speak,
p94
aVand then his food and his drink and his relatives turned to gold
p95
aVand he died in misery and starvation.
p96
aVSo we'll call this "the King Midas problem"
p97
aVof stating an objective which is not, in fact,
p98
aVtruly aligned with what we want.
p99
aVIn modern terms, we call this "the value alignment problem."
p100
aa(lp101
VPutting in the wrong objective is not the only part of the problem.
p102
aVThere's another part.
p103
aVIf you put an objective into a machine,
p104
aVeven something as simple as, "Fetch the coffee,"
p105
aVthe machine says to itself,
p106
aV"Well, how might I fail to fetch the coffee?
p107
aVSomeone might switch me off.
p108
aVOK, I have to take steps to prevent that.
p109
aVI will disable my 'off' switch.
p110
aVI will do anything to defend myself against interference
p111
aVwith this objective that I have been given."
p112
aVSo this single-minded pursuit
p113
aVin a very defensive mode of an objective that is, in fact,
p114
aVnot aligned with the true objectives of the human race \u2014
p115
aVthat's the problem that we face.
p116
aVAnd in fact, that's the high-value takeaway from this talk.
p117
aVIf you want to remember one thing,
p118
aVit's that you can't fetch the coffee if you're dead.
p119
aa(lp120
V(Laughter)
p121
aa(lp122
VIt's very simple. Just remember that. Repeat it to yourself three times a day.
p123
aa(lp124
V(Laughter)
p125
aa(lp126
VAnd in fact, this is exactly the plot
p127
aVof "2001: [A Space Odyssey]"
p128
aVHAL has an objective, a mission,
p129
aVwhich is not aligned with the objectives of the humans,
p130
aVand that leads to this conflict.
p131
aVNow fortunately, HAL is not superintelligent.
p132
aVHe's pretty smart, but eventually Dave outwits him
p133
aVand manages to switch him off.
p134
aVBut we might not be so lucky.
p135
aVSo what are we going to do?
p136
aa(lp137
VI'm trying to redefine AI
p138
aVto get away from this classical notion
p139
aVof machines that intelligently pursue objectives.
p140
aVThere are three principles involved.
p141
aVThe first one is a principle of altruism, if you like,
p142
aVthat the robot's only objective
p143
aVis to maximize the realization of human objectives,
p144
aVof human values.
p145
aVAnd by values here I don't mean touchy-feely, goody-goody values.
p146
aVI just mean whatever it is that the human would prefer
p147
aVtheir life to be like.
p148
aVAnd so this actually violates Asimov's law
p149
aVthat the robot has to protect its own existence.
p150
aVIt has no interest in preserving its existence whatsoever.
p151
aa(lp152
VThe second law is a law of humility, if you like.
p153
aVAnd this turns out to be really important to make robots safe.
p154
aVIt says that the robot does not know
p155
aVwhat those human values are,
p156
aVso it has to maximize them, but it doesn't know what they are.
p157
aVAnd that avoids this problem of single-minded pursuit
p158
aVof an objective.
p159
aVThis uncertainty turns out to be crucial.
p160
aa(lp161
VNow, in order to be useful to us,
p162
aVit has to have some idea of what we want.
p163
aVIt obtains that information primarily by observation of human choices,
p164
aVso our own choices reveal information
p165
aVabout what it is that we prefer our lives to be like.
p166
aVSo those are the three principles.
p167
aVLet's see how that applies to this question of:
p168
aV"Can you switch the machine off?" as Turing suggested.
p169
aa(lp170
VSo here's a PR2 robot.
p171
aVThis is one that we have in our lab,
p172
aVand it has a big red "off" switch right on the back.
p173
aVThe question is: Is it going to let you switch it off?
p174
aVIf we do it the classical way,
p175
aVwe give it the objective of, "Fetch the coffee, I must fetch the coffee,
p176
aVI can't fetch the coffee if I'm dead,"
p177
aVso obviously the PR2 has been listening to my talk,
p178
aVand so it says, therefore, "I must disable my 'off' switch,
p179
aVand probably taser all the other people in Starbucks
p180
aVwho might interfere with me."
p181
aa(lp182
V(Laughter)
p183
aa(lp184
VSo this seems to be inevitable, right?
p185
aVThis kind of failure mode seems to be inevitable,
p186
aVand it follows from having a concrete, definite objective.
p187
aa(lp188
VSo what happens if the machine is uncertain about the objective?
p189
aVWell, it reasons in a different way.
p190
aVIt says, "OK, the human might switch me off,
p191
aVbut only if I'm doing something wrong.
p192
aVWell, I don't really know what wrong is,
p193
aVbut I know that I don't want to do it."
p194
aVSo that's the first and second principles right there.
p195
aV"So I should let the human switch me off."
p196
aVAnd in fact you can calculate the incentive that the robot has
p197
aVto allow the human to switch it off,
p198
aVand it's directly tied to the degree
p199
aVof uncertainty about the underlying objective.
p200
aa(lp201
VAnd then when the machine is switched off,
p202
aVthat third principle comes into play.
p203
aVIt learns something about the objectives it should be pursuing,
p204
aVbecause it learns that what it did wasn't right.
p205
aVIn fact, we can, with suitable use of Greek symbols,
p206
aVas mathematicians usually do,
p207
aVwe can actually prove a theorem
p208
aVthat says that such a robot is provably beneficial to the human.
p209
aVYou are provably better off with a machine that's designed in this way
p210
aVthan without it.
p211
aVSo this is a very simple example, but this is the first step
p212
aVin what we're trying to do with human-compatible AI.
p213
aa(lp214
VNow, this third principle,
p215
aVI think is the one that you're probably scratching your head over.
p216
aVYou're probably thinking, "Well, you know, I behave badly.
p217
aVI don't want my robot to behave like me.
p218
aVI sneak down in the middle of the night and take stuff from the fridge.
p219
aVI do this and that."
p220
aVThere's all kinds of things you don't want the robot doing.
p221
aVBut in fact, it doesn't quite work that way.
p222
aVJust because you behave badly
p223
aVdoesn't mean the robot is going to copy your behavior.
p224
aVIt's going to understand your motivations and maybe help you resist them,
p225
aVif appropriate.
p226
aVBut it's still difficult.
p227
aVWhat we're trying to do, in fact,
p228
aVis to allow machines to predict for any person and for any possible life
p229
aVthat they could live,
p230
aVand the lives of everybody else:
p231
aVWhich would they prefer?
p232
aVAnd there are many, many difficulties involved in doing this;
p233
aVI don't expect that this is going to get solved very quickly.
p234
aVThe real difficulties, in fact, are us.
p235
aa(lp236
VAs I have already mentioned, we behave badly.
p237
aVIn fact, some of us are downright nasty.
p238
aVNow the robot, as I said, doesn't have to copy the behavior.
p239
aVThe robot does not have any objective of its own.
p240
aVIt's purely altruistic.
p241
aVAnd it's not designed just to satisfy the desires of one person, the user,
p242
aVbut in fact it has to respect the preferences of everybody.
p243
aVSo it can deal with a certain amount of nastiness,
p244
aVand it can even understand that your nastiness, for example,
p245
aVyou may take bribes as a passport official
p246
aVbecause you need to feed your family and send your kids to school.
p247
aVIt can understand that; it doesn't mean it's going to steal.
p248
aVIn fact, it'll just help you send your kids to school.
p249
aa(lp250
VWe are also computationally limited.
p251
aVLee Sedol is a brilliant Go player,
p252
aVbut he still lost.
p253
aVSo if we look at his actions, he took an action that lost the game.
p254
aVThat doesn't mean he wanted to lose.
p255
aVSo to understand his behavior,
p256
aVwe actually have to invert through a model of human cognition
p257
aVthat includes our computational limitations \u2014 a very complicated model.
p258
aVBut it's still something that we can work on understanding.
p259
aa(lp260
VProbably the most difficult part, from my point of view as an AI researcher,
p261
aVis the fact that there are lots of us,
p262
aVand so the machine has to somehow trade off, weigh up the preferences
p263
aVof many different people,
p264
aVand there are different ways to do that.
p265
aVEconomists, sociologists, moral philosophers have understood that,
p266
aVand we are actively looking for collaboration.
p267
aa(lp268
VLet's have a look and see what happens when you get that wrong.
p269
aVSo you can have a conversation, for example,
p270
aVwith your intelligent personal assistant
p271
aVthat might be available in a few years' time.
p272
aVThink of a Siri on steroids.
p273
aVSo Siri says, "Your wife called to remind you about dinner tonight."
p274
aVAnd of course, you've forgotten. "What? What dinner?
p275
aVWhat are you talking about?"
p276
aa(lp277
V"Uh, your 20th anniversary at 7pm."
p278
aa(lp279
V"I can't do that. I'm meeting with the secretary-general at 7:30.
p280
aVHow could this have happened?"
p281
aa(lp282
V"Well, I did warn you, but you overrode my recommendation."
p283
aa(lp284
V"Well, what am I going to do? I can't just tell him I'm too busy."
p285
aa(lp286
V"Don't worry. I arranged for his plane to be delayed."
p287
aa(lp288
V(Laughter)
p289
aa(lp290
V"Some kind of computer malfunction."
p291
aa(lp292
V(Laughter)
p293
aa(lp294
V"Really? You can do that?"
p295
aa(lp296
V"He sends his profound apologies
p297
aVand looks forward to meeting you for lunch tomorrow."
p298
aa(lp299
V(Laughter)
p300
aa(lp301
VSo the values here \u2014 there's a slight mistake going on.
p302
aVThis is clearly following my wife's values
p303
aVwhich is "Happy wife, happy life."
p304
aa(lp305
V(Laughter)
p306
aa(lp307
VIt could go the other way.
p308
aVYou could come home after a hard day's work,
p309
aVand the computer says, "Long day?"
p310
aa(lp311
V"Yes, I didn't even have time for lunch."
p312
aa(lp313
V"You must be very hungry."
p314
aa(lp315
V"Starving, yeah. Could you make some dinner?"
p316
aa(lp317
V"There's something I need to tell you."
p318
aa(lp319
V(Laughter)
p320
aa(lp321
V"There are humans in South Sudan who are in more urgent need than you."
p322
aa(lp323
V(Laughter)
p324
aa(lp325
V"So I'm leaving. Make your own dinner."
p326
aa(lp327
V(Laughter)
p328
aa(lp329
VSo we have to solve these problems,
p330
aVand I'm looking forward to working on them.
p331
aa(lp332
VThere are reasons for optimism.
p333
aVOne reason is,
p334
aVthere is a massive amount of data.
p335
aVBecause remember \u2014 I said they're going to read everything
p336
aVthe human race has ever written.
p337
aVMost of what we write about is human beings doing things
p338
aVand other people getting upset about it.
p339
aVSo there's a massive amount of data to learn from.
p340
aa(lp341
VThere's also a very strong economic incentive
p342
aVto get this right.
p343
aVSo imagine your domestic robot's at home.
p344
aVYou're late from work again and the robot has to feed the kids,
p345
aVand the kids are hungry and there's nothing in the fridge.
p346
aVAnd the robot sees the cat.
p347
aa(lp348
V(Laughter)
p349
aa(lp350
VAnd the robot hasn't quite learned the human value function properly,
p351
aVso it doesn't understand
p352
aVthe sentimental value of the cat outweighs the nutritional value of the cat.
p353
aa(lp354
V(Laughter)
p355
aa(lp356
VSo then what happens?
p357
aVWell, it happens like this:
p358
aV"Deranged robot cooks kitty for family dinner."
p359
aVThat one incident would be the end of the domestic robot industry.
p360
aVSo there's a huge incentive to get this right
p361
aVlong before we reach superintelligent machines.
p362
aa(lp363
VSo to summarize:
p364
aVI'm actually trying to change the definition of AI
p365
aVso that we have provably beneficial machines.
p366
aVAnd the principles are:
p367
aVmachines that are altruistic,
p368
aVthat want to achieve only our objectives,
p369
aVbut that are uncertain about what those objectives are,
p370
aVand will watch all of us
p371
aVto learn more about what it is that we really want.
p372
aVAnd hopefully in the process, we will learn to be better people.
p373
aVThank you very much.
p374
aa(lp375
V(Applause)
p376
aa(lp377
VChris Anderson: So interesting, Stuart.
p378
aVWe're going to stand here a bit because I think they're setting up
p379
aVfor our next speaker.
p380
aa(lp381
VA couple of questions.
p382
aVSo the idea of programming in ignorance seems intuitively really powerful.
p383
aVAs you get to superintelligence,
p384
aVwhat's going to stop a robot
p385
aVreading literature and discovering this idea that knowledge
p386
aVis actually better than ignorance
p387
aVand still just shifting its own goals and rewriting that programming?
p388
aa(lp389
VStuart Russell: Yes, so we want it to learn more, as I said,
p390
aVabout our objectives.
p391
aVIt'll only become more certain as it becomes more correct,
p392
aVso the evidence is there
p393
aVand it's going to be designed to interpret it correctly.
p394
aVIt will understand, for example, that books are very biased
p395
aVin the evidence they contain.
p396
aVThey only talk about kings and princes
p397
aVand elite white male people doing stuff.
p398
aVSo it's a complicated problem,
p399
aVbut as it learns more about our objectives
p400
aVit will become more and more useful to us.
p401
aa(lp402
VCA: And you couldn't just boil it down to one law,
p403
aVyou know, hardwired in:
p404
aV"if any human ever tries to switch me off,
p405
aVI comply. I comply."
p406
aa(lp407
VSR: Absolutely not.
p408
aVThat would be a terrible idea.
p409
aVSo imagine that you have a self-driving car
p410
aVand you want to send your five-year-old
p411
aVoff to preschool.
p412
aVDo you want your five-year-old to be able to switch off the car
p413
aVwhile it's driving along?
p414
aVProbably not.
p415
aVSo it needs to understand how rational and sensible the person is.
p416
aVThe more rational the person,
p417
aVthe more willing you are to be switched off.
p418
aVIf the person is completely random or even malicious,
p419
aVthen you're less willing to be switched off.
p420
aa(lp421
VCA: All right. Stuart, can I just say,
p422
aVI really, really hope you figure this out for us.
p423
aVThank you so much for that talk. That was amazing.
p424
aa(lp425
VSR: Thank you.
p426
aa(lp427
V(Applause)
p428
aasS'id'
p429
I2781
sS'title'
p430
V3 principles for creating safer AI
p431
s.