(dp0
S'transcript'
p1
(lp2
(lp3
VToday I'm going to talk about technology and society.
p4
aVThe Department of Transport estimated that last year
p5
aV35,000 people died from traffic crashes in the US alone.
p6
aVWorldwide, 1.2 million people die every year in traffic accidents.
p7
aVIf there was a way we could eliminate 90 percent of those accidents,
p8
aVwould you support it?
p9
aVOf course you would.
p10
aVThis is what driverless car technology promises to achieve
p11
aVby eliminating the main source of accidents \u2014
p12
aVhuman error.
p13
aa(lp14
VNow picture yourself in a driverless car in the year 2030,
p15
aVsitting back and watching this vintage TEDxCambridge video.
p16
aa(lp17
V(Laughter)
p18
aa(lp19
VAll of a sudden,
p20
aVthe car experiences mechanical failure and is unable to stop.
p21
aVIf the car continues,
p22
aVit will crash into a bunch of pedestrians crossing the street,
p23
aVbut the car may swerve,
p24
aVhitting one bystander,
p25
aVkilling them to save the pedestrians.
p26
aVWhat should the car do, and who should decide?
p27
aVWhat if instead the car could swerve into a wall,
p28
aVcrashing and killing you, the passenger,
p29
aVin order to save those pedestrians?
p30
aVThis scenario is inspired by the trolley problem,
p31
aVwhich was invented by philosophers a few decades ago
p32
aVto think about ethics.
p33
aa(lp34
VNow, the way we think about this problem matters.
p35
aVWe may for example not think about it at all.
p36
aVWe may say this scenario is unrealistic,
p37
aVincredibly unlikely, or just silly.
p38
aVBut I think this criticism misses the point
p39
aVbecause it takes the scenario too literally.
p40
aVOf course no accident is going to look like this;
p41
aVno accident has two or three options
p42
aVwhere everybody dies somehow.
p43
aVInstead, the car is going to calculate something
p44
aVlike the probability of hitting a certain group of people,
p45
aVif you swerve one direction versus another direction,
p46
aVyou might slightly increase the risk to passengers or other drivers
p47
aVversus pedestrians.
p48
aVIt's going to be a more complex calculation,
p49
aVbut it's still going to involve trade-offs,
p50
aVand trade-offs often require ethics.
p51
aa(lp52
VWe might say then, "Well, let's not worry about this.
p53
aVLet's wait until technology is fully ready and 100 percent safe."
p54
aVSuppose that we can indeed eliminate 90 percent of those accidents,
p55
aVor even 99 percent in the next 10 years.
p56
aVWhat if eliminating the last one percent of accidents
p57
aVrequires 50 more years of research?
p58
aVShould we not adopt the technology?
p59
aVThat's 60 million people dead in car accidents
p60
aVif we maintain the current rate.
p61
aVSo the point is,
p62
aVwaiting for full safety is also a choice,
p63
aVand it also involves trade-offs.
p64
aa(lp65
VPeople online on social media have been coming up with all sorts of ways
p66
aVto not think about this problem.
p67
aVOne person suggested the car should just swerve somehow
p68
aVin between the passengers \u2014
p69
aa(lp70
V(Laughter)
p71
aa(lp72
Vand the bystander.
p73
aVOf course if that's what the car can do, that's what the car should do.
p74
aVWe're interested in scenarios in which this is not possible.
p75
aVAnd my personal favorite was a suggestion by a blogger
p76
aVto have an eject button in the car that you press \u2014
p77
aa(lp78
V(Laughter)
p79
aa(lp80
Vjust before the car self-destructs.
p81
aa(lp82
V(Laughter)
p83
aa(lp84
VSo if we acknowledge that cars will have to make trade-offs on the road,
p85
aVhow do we think about those trade-offs,
p86
aVand how do we decide?
p87
aVWell, maybe we should run a survey to find out what society wants,
p88
aVbecause ultimately,
p89
aVregulations and the law are a reflection of societal values.
p90
aa(lp91
VSo this is what we did.
p92
aVWith my collaborators,
p93
aVJean-François Bonnefon and Azim Shariff,
p94
aVwe ran a survey
p95
aVin which we presented people with these types of scenarios.
p96
aVWe gave them two options inspired by two philosophers:
p97
aVJeremy Bentham and Immanuel Kant.
p98
aVBentham says the car should follow utilitarian ethics:
p99
aVit should take the action that will minimize total harm \u2014
p100
aVeven if that action will kill a bystander
p101
aVand even if that action will kill the passenger.
p102
aVImmanuel Kant says the car should follow duty-bound principles,
p103
aVlike "Thou shalt not kill."
p104
aVSo you should not take an action that explicitly harms a human being,
p105
aVand you should let the car take its course
p106
aVeven if that's going to harm more people.
p107
aa(lp108
VWhat do you think?
p109
aVBentham or Kant?
p110
aVHere's what we found.
p111
aVMost people sided with Bentham.
p112
aVSo it seems that people want cars to be utilitarian,
p113
aVminimize total harm,
p114
aVand that's what we should all do.
p115
aVProblem solved.
p116
aVBut there is a little catch.
p117
aVWhen we asked people whether they would purchase such cars,
p118
aVthey said, "Absolutely not."
p119
aa(lp120
V(Laughter)
p121
aa(lp122
VThey would like to buy cars that protect them at all costs,
p123
aVbut they want everybody else to buy cars that minimize harm.
p124
aa(lp125
V(Laughter)
p126
aa(lp127
VWe've seen this problem before.
p128
aVIt's called a social dilemma.
p129
aVAnd to understand the social dilemma,
p130
aVwe have to go a little bit back in history.
p131
aVIn the 1800s,
p132
aVEnglish economist William Forster Lloyd published a pamphlet
p133
aVwhich describes the following scenario.
p134
aVYou have a group of farmers \u2014
p135
aVEnglish farmers \u2014
p136
aVwho are sharing a common land for their sheep to graze.
p137
aVNow, if each farmer brings a certain number of sheep \u2014
p138
aVlet's say three sheep \u2014
p139
aVthe land will be rejuvenated,
p140
aVthe farmers are happy,
p141
aVthe sheep are happy,
p142
aVeverything is good.
p143
aVNow, if one farmer brings one extra sheep,
p144
aVthat farmer will do slightly better, and no one else will be harmed.
p145
aVBut if every farmer made that individually rational decision,
p146
aVthe land will be overrun, and it will be depleted
p147
aVto the detriment of all the farmers,
p148
aVand of course, to the detriment of the sheep.
p149
aa(lp150
VWe see this problem in many places:
p151
aVin the difficulty of managing overfishing,
p152
aVor in reducing carbon emissions to mitigate climate change.
p153
aVWhen it comes to the regulation of driverless cars,
p154
aVthe common land now is basically public safety \u2014
p155
aVthat's the common good \u2014
p156
aVand the farmers are the passengers
p157
aVor the car owners who are choosing to ride in those cars.
p158
aVAnd by making the individually rational choice
p159
aVof prioritizing their own safety,
p160
aVthey may collectively be diminishing the common good,
p161
aVwhich is minimizing total harm.
p162
aVIt's called the tragedy of the commons,
p163
aVtraditionally,
p164
aVbut I think in the case of driverless cars,
p165
aVthe problem may be a little bit more insidious
p166
aVbecause there is not necessarily an individual human being
p167
aVmaking those decisions.
p168
aVSo car manufacturers may simply program cars
p169
aVthat will maximize safety for their clients,
p170
aVand those cars may learn automatically on their own
p171
aVthat doing so requires slightly increasing risk for pedestrians.
p172
aVSo to use the sheep metaphor,
p173
aVit's like we now have electric sheep that have a mind of their own.
p174
aa(lp175
V(Laughter)
p176
aa(lp177
VAnd they may go and graze even if the farmer doesn't know it.
p178
aa(lp179
VSo this is what we may call the tragedy of the algorithmic commons,
p180
aVand if offers new types of challenges.
p181
aVTypically, traditionally,
p182
aVwe solve these types of social dilemmas using regulation,
p183
aVso either governments or communities get together,
p184
aVand they decide collectively what kind of outcome they want
p185
aVand what sort of constraints on individual behavior
p186
aVthey need to implement.
p187
aVAnd then using monitoring and enforcement,
p188
aVthey can make sure that the public good is preserved.
p189
aVSo why don't we just,
p190
aVas regulators,
p191
aVrequire that all cars minimize harm?
p192
aVAfter all, this is what people say they want.
p193
aVAnd more importantly,
p194
aVI can be sure that as an individual,
p195
aVif I buy a car that may sacrifice me in a very rare case,
p196
aVI'm not the only sucker doing that
p197
aVwhile everybody else enjoys unconditional protection.
p198
aa(lp199
VIn our survey, we did ask people whether they would support regulation
p200
aVand here's what we found.
p201
aVFirst of all, people said no to regulation;
p202
aVand second, they said,
p203
aV"Well if you regulate cars to do this and to minimize total harm,
p204
aVI will not buy those cars."
p205
aVSo ironically,
p206
aVby regulating cars to minimize harm,
p207
aVwe may actually end up with more harm
p208
aVbecause people may not opt into the safer technology
p209
aVeven if it's much safer than human drivers.
p210
aa(lp211
VI don't have the final answer to this riddle,
p212
aVbut I think as a starting point,
p213
aVwe need society to come together
p214
aVto decide what trade-offs we are comfortable with
p215
aVand to come up with ways in which we can enforce those trade-offs.
p216
aa(lp217
VAs a starting point, my brilliant students,
p218
aVEdmond Awad and Sohan Dsouza,
p219
aVbuilt the Moral Machine website,
p220
aVwhich generates random scenarios at you \u2014
p221
aVbasically a bunch of random dilemmas in a sequence
p222
aVwhere you have to choose what the car should do in a given scenario.
p223
aVAnd we vary the ages and even the species of the different victims.
p224
aVSo far we've collected over five million decisions
p225
aVby over one million people worldwide
p226
aVfrom the website.
p227
aVAnd this is helping us form an early picture
p228
aVof what trade-offs people are comfortable with
p229
aVand what matters to them \u2014
p230
aVeven across cultures.
p231
aVBut more importantly,
p232
aVdoing this exercise is helping people recognize
p233
aVthe difficulty of making those choices
p234
aVand that the regulators are tasked with impossible choices.
p235
aVAnd maybe this will help us as a society understand the kinds of trade-offs
p236
aVthat will be implemented ultimately in regulation.
p237
aa(lp238
VAnd indeed, I was very happy to hear
p239
aVthat the first set of regulations
p240
aVthat came from the Department of Transport \u2014
p241
aVannounced last week \u2014
p242
aVincluded a 15-point checklist for all carmakers to provide,
p243
aVand number 14 was ethical consideration \u2014
p244
aVhow are you going to deal with that.
p245
aVWe also have people reflect on their own decisions
p246
aVby giving them summaries of what they chose.
p247
aVI'll give you one example \u2014
p248
aVI'm just going to warn you that this is not your typical example,
p249
aVyour typical user.
p250
aVThis is the most sacrificed and the most saved character for this person.
p251
aa(lp252
V(Laughter)
p253
aa(lp254
VSome of you may agree with him,
p255
aVor her, we don't know.
p256
aVBut this person also seems to slightly prefer passengers over pedestrians
p257
aVin their choices
p258
aVand is very happy to punish jaywalking.
p259
aa(lp260
V(Laughter)
p261
aa(lp262
VSo let's wrap up.
p263
aVWe started with the question \u2014 let's call it the ethical dilemma \u2014
p264
aVof what the car should do in a specific scenario:
p265
aVswerve or stay?
p266
aVBut then we realized that the problem was a different one.
p267
aVIt was the problem of how to get society to agree on and enforce
p268
aVthe trade-offs they're comfortable with.
p269
aVIt's a social dilemma.
p270
aa(lp271
VIn the 1940s, Isaac Asimov wrote his famous laws of robotics \u2014
p272
aVthe three laws of robotics.
p273
aVA robot may not harm a human being,
p274
aVa robot may not disobey a human being,
p275
aVand a robot may not allow itself to come to harm \u2014
p276
aVin this order of importance.
p277
aVBut after 40 years or so
p278
aVand after so many stories pushing these laws to the limit,
p279
aVAsimov introduced the zeroth law
p280
aVwhich takes precedence above all,
p281
aVand it's that a robot may not harm humanity as a whole.
p282
aVI don't know what this means in the context of driverless cars
p283
aVor any specific situation,
p284
aVand I don't know how we can implement it,
p285
aVbut I think that by recognizing
p286
aVthat the regulation of driverless cars is not only a technological problem
p287
aVbut also a societal cooperation problem,
p288
aVI hope that we can at least begin to ask the right questions.
p289
aa(lp290
VThank you.
p291
aa(lp292
V(Applause)
p293
aasS'id'
p294
I2824
sS'title'
p295
VWhat moral decisions should driverless cars make?
p296
s.